{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "with open('adhoc/lrec-train-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        train[idx] = sample_dict\n",
    "        idx += 1\n",
    "with open('adhoc/lrec-test-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        test[idx] = sample_dict\n",
    "        idx += 1\n",
    "\n",
    "lrec = {'train': train, 'test': test}\n",
    "with open('lrec_split.json', 'w') as f:\n",
    "    json.dump(lrec, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "with open('adhoc/arc-train-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        train[idx] = sample_dict\n",
    "        idx += 1\n",
    "with open('adhoc/arc-test-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        test[idx] = sample_dict\n",
    "        idx += 1\n",
    "\n",
    "arc = {'train': train, 'test': test}\n",
    "with open('arc_split.json', 'w') as f:\n",
    "    json.dump(arc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "with open('adhoc/ARG.txt', 'r') as f1:\n",
    "    with open('adhoc/ARG_clean.csv', 'r') as f2:\n",
    "        reader = csv.reader(f2)\n",
    "        data = list(reader)\n",
    "        for line in f1:\n",
    "            sample_dict = {}\n",
    "            split_line = line.split('\\t')\n",
    "\n",
    "            sample_dict['text'] = data[int(split_line[0])][0]\n",
    "            sample_dict['label'] = int(split_line[2].rstrip('\\n'))\n",
    "            if split_line[1] == 'train':\n",
    "                train[int(split_line[0])] = sample_dict\n",
    "            else:\n",
    "                test[int(split_line[0])] = sample_dict\n",
    "\n",
    "arg = {'train': train, 'test': test}\n",
    "with open('arg_split.json', 'w') as f:\n",
    "    json.dump(arg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "with open('adhoc/arc-train-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        train[idx] = sample_dict\n",
    "        idx += 1\n",
    "with open('adhoc/arc-test-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        test[idx] = sample_dict\n",
    "        idx += 1\n",
    "\n",
    "arc = {'train': train, 'test': test}\n",
    "with open('arc_split.json', 'w') as f:\n",
    "    json.dump(arc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "with open('adhoc/nu-train-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        train[idx] = sample_dict\n",
    "        idx += 1\n",
    "with open('adhoc/nu-test-all-terms.txt', 'r') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        sample_dict = {}\n",
    "        split_line = line.split('\\t')\n",
    "        sample_dict['text'] = split_line[1].rstrip('\\n')\n",
    "        sample_dict['label'] = int(split_line[0])\n",
    "        test[idx] = sample_dict\n",
    "        idx += 1\n",
    "\n",
    "nu = {'train': train, 'test': test}\n",
    "with open('nu_split.json', 'w') as f:\n",
    "    json.dump(nu, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "# datasets = ['trec', 'lrec', 'arc', 'arg', 'nu']\n",
    "\n",
    "dataset = 'trec'\n",
    "\n",
    "with open(f'.\\preprocess\\{dataset}_split.json', 'r') as f:\n",
    "    split = json.load(f)\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "for k,v in split['train'].items():\n",
    "    text.append(v['text'])\n",
    "    label.append(v['coarse_label'])\n",
    "\n",
    "for k,v in split['test'].items():\n",
    "    text.append(v['text'])\n",
    "    label.append(v['coarse_label'])\n",
    "\n",
    "data = {'text': text, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Junyoung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Junyoung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1300\n",
      "1 1344\n",
      "0 95\n",
      "3 1288\n",
      "5 1009\n",
      "4 916\n",
      "\\begin{tabular}{llrl}\n",
      "\\hline\n",
      " Label   & Keyword      &   Frequency & Percentage   \\\\\n",
      "\\hline\n",
      " Label 2 & mean         &          68 & 5.23\\%        \\\\\n",
      " Label 2 & origin       &          54 & 4.15\\%        \\\\\n",
      " Label 2 & get          &          37 & 2.77\\%        \\\\\n",
      " Label 1 & name         &         131 & 9.75\\%        \\\\\n",
      " Label 1 & fear         &          66 & 4.91\\%        \\\\\n",
      " Label 1 & first        &          53 & 3.94\\%        \\\\\n",
      " Label 0 & stand        &          46 & 48.42\\%       \\\\\n",
      " Label 0 & abbreviation &          18 & 18.95\\%       \\\\\n",
      " Label 0 & mean         &          11 & 11.58\\%       \\\\\n",
      " Label 3 & name         &         149 & 11.57\\%       \\\\\n",
      " Label 3 & first        &          96 & 7.38\\%        \\\\\n",
      " Label 3 & president    &          75 & 5.82\\%        \\\\\n",
      " Label 5 & many         &         330 & 32.71\\%       \\\\\n",
      " Label 5 & year         &          83 & 8.23\\%        \\\\\n",
      " Label 5 & much         &          62 & 6.14\\%        \\\\\n",
      " Label 4 & country      &         126 & 13.65\\%       \\\\\n",
      " Label 4 & city         &         112 & 12.01\\%       \\\\\n",
      " Label 4 & state        &          70 & 7.21\\%        \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import json \n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# datasets = ['trec', 'lrec', 'arc', 'nu', 'bloom']\n",
    "\n",
    "dataset = 'trec'\n",
    "\n",
    "with open(f'.\\preprocess\\{dataset}_split.json', 'r') as f:\n",
    "    split = json.load(f)\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "for k,v in split['train'].items():\n",
    "    text.append(v['text'])\n",
    "    label.append(v['coarse_label'])\n",
    "\n",
    "for k,v in split['test'].items():\n",
    "    text.append(v['text'])\n",
    "    label.append(v['coarse_label'])\n",
    "\n",
    "data = {'text': text, 'label': label}\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing function to tokenize and clean text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove non-alphanumeric characters\n",
    "    clean_text = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in clean_text if not word in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "# Get top 3 keywords and their statistics for each label\n",
    "keyword_analysis = {}\n",
    "for label in df['label'].unique():\n",
    "    # Filter text by label\n",
    "    texts = df[df['label'] == label]['text']\n",
    "    \n",
    "    print(label, len(texts))\n",
    "    \n",
    "    # Tokenize and count word frequencies\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(preprocess_text(text))\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    # Get the top 3 keywords\n",
    "    top_keywords = word_counts.most_common(3)\n",
    "    keyword_analysis[label] = {\n",
    "        \"top_keywords\": top_keywords,\n",
    "        \"percentages\": {}\n",
    "    }\n",
    "    \n",
    "    # Calculate percentage of samples containing each keyword\n",
    "    for keyword, _ in top_keywords:\n",
    "        containing_samples = texts.apply(lambda x: keyword in preprocess_text(x))\n",
    "        percentage = containing_samples.sum() / len(texts) * 100\n",
    "        keyword_analysis[label][\"percentages\"][keyword] = percentage\n",
    "\n",
    "# Prepare data for the LaTeX table\n",
    "latex_table_data = []\n",
    "for label, analysis in keyword_analysis.items():\n",
    "    for keyword, count in analysis['top_keywords']:\n",
    "        percentage = analysis['percentages'][keyword]\n",
    "        latex_table_data.append([f\"Label {label}\", keyword, count, f\"{percentage:.2f}%\"])\n",
    "\n",
    "# Convert to LaTeX table\n",
    "headers = [\"Label\", \"Keyword\", \"Frequency\", \"Percentage\"]\n",
    "latex_table = tabulate(latex_table_data, headers, tablefmt=\"latex\")\n",
    "\n",
    "# Save to a .tex file\n",
    "with open(f\"{dataset}_keyword_analysis_table.tex\", \"w\") as file:\n",
    "    file.write(latex_table)\n",
    "\n",
    "# Print LaTeX table\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".shine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
