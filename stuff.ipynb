{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee-j/github/SHINE-EMNLP21/.shine/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.09k/5.09k [00:00<00:00, 9.54MB/s]\n",
      "Downloading metadata: 100%|██████████| 3.34k/3.34k [00:00<00:00, 5.83MB/s]\n",
      "Downloading readme: 100%|██████████| 10.6k/10.6k [00:00<00:00, 14.8MB/s]\n",
      "Downloading data: 100%|██████████| 336k/336k [00:00<00:00, 393kB/s]\n",
      "Downloading data: 100%|██████████| 23.4k/23.4k [00:00<00:00, 140kB/s] \n",
      "Downloading data files: 100%|██████████| 2/2 [00:05<00:00,  2.72s/it]\n",
      "Generating train split: 100%|██████████| 5452/5452 [00:00<00:00, 29770.81 examples/s]\n",
      "Generating test split: 100%|██████████| 500/500 [00:00<00:00, 24882.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_dataset(\"trec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 269.63ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "520661"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].to_json(\"./preprocess/trec_train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for split in data:\n",
    "    split_dict = {}\n",
    "    for i in range(len(data[split])):\n",
    "        split_dict[i] = data[split][i]\n",
    "    data_dict[split] = split_dict\n",
    "        \n",
    "with open(\"./preprocess/trec_split.json\", \"w\") as f:\n",
    "    json.dump(data_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5452"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penn Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/lee-j/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ruleset = set(rule for tree in nltk.corpus.treebank.parsed_sents() \n",
    "           for rule in tree.productions())\n",
    "with open(\"preprocess/cfg.txt\", \"w\") as f:\n",
    "    for rule in ruleset:\n",
    "        f.write(str(rule) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coreferee.manager.CorefereeBroker at 0x7f30d121cfa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import coreferee, spacy, spacy_transformers\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# python -m coreferee install en\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "nlp.add_pipe('coreferee')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<coreferee.manager.CorefereeBroker object at 0x000002DE8E9256D0>\n",
    ">>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: he(1), his(6), Peter(9), He(16), his(18)\n",
      "1: work(7), it(14)\n",
      "2: [He(16); wife(19)], they(21), They(26), they(31)\n",
      "3: Spain(29), country(34)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much.\")\n",
    "\n",
    "doc._.coref_chains.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: he(1), his(6), Peter(9), He(16), his(18)\n",
      "2: [He(16); wife(19)], they(21), They(26), they(31)\n"
     ]
    }
   ],
   "source": [
    "doc[16]._.coref_chains.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Peter, wife]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.coref_chains.resolve(doc[31])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [one[1].lower() for one in nltk.pos_tag(nltk.word_tokenize(\"Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much.\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'prp',\n",
       " 'vbd',\n",
       " 'rb',\n",
       " 'jj',\n",
       " 'in',\n",
       " 'prp$',\n",
       " 'nn',\n",
       " ',',\n",
       " 'nnp',\n",
       " 'vbd',\n",
       " 'vbn',\n",
       " 'rb',\n",
       " 'in',\n",
       " 'prp',\n",
       " '.',\n",
       " 'prp',\n",
       " 'cc',\n",
       " 'prp$',\n",
       " 'nn',\n",
       " 'vbd',\n",
       " 'prp',\n",
       " 'vbd',\n",
       " 'dt',\n",
       " 'nn',\n",
       " '.',\n",
       " 'prp',\n",
       " 'vbd',\n",
       " 'to',\n",
       " 'nnp',\n",
       " 'in',\n",
       " 'prp',\n",
       " 'vbd',\n",
       " 'dt',\n",
       " 'nn',\n",
       " 'rb',\n",
       " 'rb',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter two space-separated words\n",
      "hot True 74.552574 False\n",
      "warm True 59.266777 False\n",
      "Similarity: 0.5941726565361023\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "  \n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "  \n",
    "print(\"Enter two space-separated words\") \n",
    "words = input() \n",
    "  \n",
    "tokens = nlp(words) \n",
    "  \n",
    "for token in tokens: \n",
    "    # Printing the following attributes of each token. \n",
    "    # text: the word string, has_vector: if it contains \n",
    "    # a vector representation in the model,  \n",
    "    # vector_norm: the algebraic norm of the vector, \n",
    "    # is_oov: if the word is out of vocabulary. \n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
    "  \n",
    "token1, token2 = tokens[0], tokens[1] \n",
    "  \n",
    "print(\"Similarity:\", token1.similarity(token2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter two space-separated words\n",
      "warm True 59.266777 False\n",
      "cool True 46.042625 False\n",
      "Similarity: 0.6782691478729248\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter two space-separated words\") \n",
    "words = input() \n",
    "  \n",
    "tokens = nlp(words) \n",
    "  \n",
    "for token in tokens: \n",
    "    # Printing the following attributes of each token. \n",
    "    # text: the word string, has_vector: if it contains \n",
    "    # a vector representation in the model,  \n",
    "    # vector_norm: the algebraic norm of the vector, \n",
    "    # is_oov: if the word is out of vocabulary. \n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
    "  \n",
    "token1, token2 = tokens[0], tokens[1] \n",
    "  \n",
    "print(\"Similarity:\", token1.similarity(token2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee-j/github/SHINE-EMNLP21/.shine/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "docs = ['how did serfdom develop in and then leave russia ?', 'what films featured the character popeye doyle ?', 'how can I find a list of celebrities \\' real names ?']\n",
    "docs_embeddings = embedding_model.encode(docs)\n",
    "word_embeddings = embedding_model.encode(docs, output_value=\"token_embeddings\")\n",
    "\n",
    "token_ids = []\n",
    "token_strings = []\n",
    "tokenizer = embedding_model._first_module().tokenizer\n",
    "\n",
    "word_emb_set = set()\n",
    "word_emb_idx_set = set()\n",
    "\n",
    "for doc in docs:\n",
    "    ids = tokenizer.encode(doc)\n",
    "    strings = tokenizer.convert_ids_to_tokens(ids)\n",
    "    token_ids.append(ids)\n",
    "    token_strings.append(strings)\n",
    "    \n",
    "    for id in ids[1:-1]:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2129,\n",
       " 2064,\n",
       " 1045,\n",
       " 2424,\n",
       " 1037,\n",
       " 2862,\n",
       " 1997,\n",
       " 12330,\n",
       " 1005,\n",
       " 2613,\n",
       " 3415,\n",
       " 1029,\n",
       " 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 7632, 102], [101, 7592, 102], [101, 2129, 2024, 2017, 102]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]', 'hi', '[SEP]'],\n",
       " ['[CLS]', 'hello', '[SEP]'],\n",
       " ['[CLS]', 'how', 'are', 'you', '[SEP]']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedQuad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5_NIDDK_QA',\n",
       " '12_MPlusHerbsSupplements_QA',\n",
       " '1_CancerGov_QA',\n",
       " '6_NINDS_QA',\n",
       " '10_MPlus_ADAM_QA',\n",
       " '8_NHLBI_QA_XML',\n",
       " '4_MPlus_Health_Topics_QA',\n",
       " '2_GARD_QA',\n",
       " '7_SeniorHealth_QA',\n",
       " '3_GHR_QA',\n",
       " '11_MPlusDrugs_QA',\n",
       " '9_CDC_QA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "folders = os.listdir('./MedQuAD')\n",
    "folders.remove('.git')\n",
    "folders.remove('LICENSE.txt')\n",
    "folders.remove('readme.txt')\n",
    "folders.remove('__MACOSX')\n",
    "folders.remove('QA-TestSet-LiveQA-Med-Qrels-2479-Answers.zip')\n",
    "folders.remove('QA-TestSet-LiveQA-Med-Qrels-2479-Answers')\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47441\n",
      "What are the treatments for Kidney Failure: Choosing a Treatment That's Right for You ? : treatment\n",
      "What are the complications of Vesicoureteral Reflux ? : complications\n",
      "What is (are) Childhood Nephrotic Syndrome ? : information\n",
      "How to prevent Childhood Nephrotic Syndrome ? : prevention\n",
      "What are the symptoms of Anemia in Chronic Kidney Disease ? : symptoms\n",
      "What is (are) Diabetic Neuropathies: The Nerve Damage of Diabetes ? : information\n",
      "What is (are) Diabetic Neuropathies: The Nerve Damage of Diabetes ? : information\n",
      "What are the symptoms of Prevent diabetes problems: Keep your kidneys healthy ? : symptoms\n",
      "Who is at risk for What I need to know about Hepatitis C? ? : susceptibility\n",
      "What to do for What I need to know about Hepatitis C ? : considerations\n",
      "What to do for What I need to know about Gas ? : considerations\n",
      "What to do for Proteinuria ? : considerations\n",
      "What are the treatments for Foodborne Illnesses ? : treatment\n",
      "What are the treatments for Nonalcoholic Steatohepatitis ? : treatment\n",
      "What is (are) Gallstones ? : information\n",
      "What causes Ectopic Kidney ? : causes\n",
      "What is (are) Primary Sclerosing Cholangitis ? : information\n",
      "What to do for Primary Sclerosing Cholangitis ? : considerations\n",
      "Who is at risk for Hepatitis B: What Asian and Pacific Islander Americans Need to Know? ? : susceptibility\n",
      "What are the symptoms of Celiac Disease ? : symptoms\n",
      "How to prevent Am I at Risk for Type 2 Diabetes? Taking Steps to Lower Your Risk of Getting Diabetes ? : prevention\n",
      "What is (are) What I need to know about Gestational Diabetes ? : information\n",
      "How to diagnose What I need to know about Gestational Diabetes ? : exams and tests\n",
      "What are the treatments for Goodpasture Syndrome ? : treatment\n",
      "What are the treatments for Hashimoto's Disease ? : treatment\n",
      "What to do for What I need to know about Lactose Intolerance ? : considerations\n",
      "What causes Simple Kidney Cysts ? : causes\n",
      "What causes Peyronie's Disease ? : causes\n",
      "What to do for Peyronie's Disease ? : considerations\n",
      "What is (are) Urinary Incontinence in Children ? : information\n",
      "What is (are) Diagnosis of Diabetes and Prediabetes ? : information\n",
      "What is (are) Whipple Disease ? : information\n",
      "How to prevent Whipple Disease ? : prevention\n",
      "What causes Adrenal Insufficiency and Addison's Disease ? : causes\n",
      "What to do for Adrenal Insufficiency and Addison's Disease ? : considerations\n",
      "What to do for Hemochromatosis ? : considerations\n",
      "What is (are) Nutrition for Advanced Chronic Kidney Disease in Adults ? : information\n",
      "What is (are) Urine Blockage in Newborns ? : information\n",
      "What to do for Urine Blockage in Newborns ? : considerations\n",
      "How to prevent What I need to know about Erectile Dysfunction ? : prevention\n",
      "What are the symptoms of National Hormone and Pituitary Program (NHPP): Information for People Treated with Pituitary Human Growth Hormone (Comprehensive Report) ? : symptoms\n",
      "What is (are) National Hormone and Pituitary Program (NHPP): Information for People Treated with Pituitary Human Growth Hormone (Comprehensive Report) ? : information\n",
      "How to diagnose Diverticular Disease ? : exams and tests\n"
     ]
    }
   ],
   "source": [
    "qns = []\n",
    "labels = []\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(f'./MedQuAD/{folder}')\n",
    "    for file in files:\n",
    "        try:\n",
    "            tree = ET.parse(f'./MedQuAD/{folder}/{file}')\n",
    "        except:\n",
    "            print(folder, file)\n",
    "        root = tree.getroot()\n",
    "        qapairs = root.find('./QAPairs')\n",
    "        try:\n",
    "            for qapair in qapairs:\n",
    "                for item in qapair:\n",
    "                    if item.tag == \"Question\":\n",
    "                        qns.append(item.text)\n",
    "                        labels.append(item.attrib['qtype'])\n",
    "        except:\n",
    "            print(folder, file)\n",
    "            \n",
    "assert len(qns) == len(labels)\n",
    "print(len(qns))\n",
    "        \n",
    "for i in range(200,500,7):\n",
    "    print(qns[i], ':', labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels_int = le.transform(labels)\n",
    "max(labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['brand names', 'brand names of combination products', 'causes',\n",
       "       'complications', 'considerations', 'contraindication', 'dietary',\n",
       "       'dose', 'emergency or overdose', 'exams and tests',\n",
       "       'forget a dose', 'frequency', 'genetic changes',\n",
       "       'how can i learn more', 'how does it work', 'how effective is it',\n",
       "       'important warning', 'indication', 'information', 'inheritance',\n",
       "       'interactions with foods',\n",
       "       'interactions with herbs and supplements',\n",
       "       'interactions with medications', 'other information', 'outlook',\n",
       "       'precautions', 'prevention', 'research', 'severe reaction',\n",
       "       'side effects', 'stages', 'storage and disposal', 'support groups',\n",
       "       'susceptibility', 'symptoms', 'treatment', 'usage',\n",
       "       'when to contact a medical professional', 'why get vaccinated'],\n",
       "      dtype='<U39')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(qns, labels_int, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "with open('medquad_split_json', 'w') as f:\n",
    "    for i in range(len(X_train)):\n",
    "        sample_dict = {}\n",
    "        sample_dict['text'] = X_train[i]\n",
    "        sample_dict['label'] = str(y_train[i])\n",
    "        train[i] = sample_dict\n",
    "    for i in range(len(X_test)):\n",
    "        sample_dict = {}\n",
    "        sample_dict['text'] = X_test[i]\n",
    "        sample_dict['label'] = str(y_test[i])\n",
    "        test[i] = sample_dict\n",
    "    medquad = {'train': train, 'test': test}\n",
    "    json.dump(medquad, f, indent=2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".shine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
