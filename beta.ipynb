{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocess/trec_split.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_data = data['train']\n",
    "test_data = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'How did serfdom develop in and then leave Russia ?',\n",
       " 'coarse_label': 2,\n",
       " 'fine_label': 26}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to form any kinds of graph, you first need to go through the entire corpus and obtain {idx:node}. we first do this for word, then pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaning text\n",
    "import string\n",
    "def clean_str(sentence ,use=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    if not use: return sentence\n",
    "\n",
    "    sentence = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" \\'s\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" \\'ve\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" n\\'t\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" \\'re\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" \\'d\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" \\'ll\", sentence)\n",
    "    sentence = re.sub(r\",\", \" , \", sentence)\n",
    "    sentence = re.sub(r\"!\", \" ! \", sentence)\n",
    "    sentence = re.sub(r\"\\(\", \" \\( \", sentence)\n",
    "    sentence = re.sub(r\"\\)\", \" \\) \", sentence)\n",
    "    sentence = re.sub(r\"\\?\", \" \\? \", sentence)\n",
    "    sentence = re.sub(r\"\\s{2,}\", \" \", sentence)\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return sentence.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI Adjacency Matrix:\n",
      "[[0.         2.83321334 2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.83321334 0.         2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.14006616 2.14006616 0.         2.14006616 2.14006616 2.14006616]\n",
      " [1.73460106 1.73460106 2.14006616 0.         1.73460106 1.73460106]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 0.         2.83321334]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 2.83321334 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    unique_words = set()\n",
    "    word_count = Counter()\n",
    "    pair_count = defaultdict(int)\n",
    "    total_words = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        line = clean_str(line)\n",
    "        words = line.split()\n",
    "        total_words += len(words)\n",
    "        word_count.update(words)\n",
    "        for i, word in enumerate(words):\n",
    "            unique_words.add(word)\n",
    "            for j in range(i + 1, len(words)):\n",
    "                pair = tuple(sorted([word, words[j]]))\n",
    "                pair_count[pair] += 1\n",
    "    \n",
    "    word_prob = {word: count / total_words for word, count in word_count.items()}\n",
    "    pair_prob = {pair: count / total_words for pair, count in pair_count.items()}\n",
    "    \n",
    "    return word_prob, pair_prob, unique_words\n",
    "\n",
    "def calculate_pmi(word_prob, pair_prob, word1, word2):\n",
    "    pair = tuple(sorted([word1, word2]))\n",
    "    if pair in pair_prob and word1 in word_prob and word2 in word_prob:\n",
    "        pmi = log(pair_prob[pair] / (word_prob[word1] * word_prob[word2]))\n",
    "        return pmi\n",
    "    return 0.0\n",
    "\n",
    "def create_pmi_matrix(sentence, word_prob, pair_prob, word_index):\n",
    "    words = clean_str(sentence).split()\n",
    "    n = len(words)\n",
    "    pmi_matrix = np.zeros((n, n))\n",
    "    node_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            node_list.append(word_index[word])\n",
    "        else:\n",
    "            node_list.append(-1)\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pmi = calculate_pmi(word_prob, pair_prob, words[i], words[j])\n",
    "            pmi_matrix[i, j] = pmi\n",
    "            pmi_matrix[j, i] = pmi  # PMI matrix is symmetric\n",
    "    \n",
    "    return pmi_matrix, node_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"Another line; with more: punctuation.\",\n",
    "    \"Is this working? Yes, it is!\"\n",
    "]\n",
    "\n",
    "word_prob, pair_prob, unique_words = process_corpus(corpus)\n",
    "word_index = {word: index for index, word in enumerate(sorted(unique_words))}\n",
    "\n",
    "sentence = \"Hello world, this is a test\"\n",
    "pmi_matrix, node_list = create_pmi_matrix(sentence, word_prob, pair_prob, word_index)\n",
    "print(\"PMI Adjacency Matrix:\")\n",
    "print(pmi_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'another': 1,\n",
       " 'hello': 2,\n",
       " 'is': 3,\n",
       " 'it': 4,\n",
       " 'line': 5,\n",
       " 'more': 6,\n",
       " 'punctuation': 7,\n",
       " 'test': 8,\n",
       " 'this': 9,\n",
       " 'with': 10,\n",
       " 'working': 11,\n",
       " 'world': 12,\n",
       " 'yes': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI Adjacency Matrix:\n",
      "[[0.         2.83321334 2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.83321334 0.         2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.14006616 2.14006616 0.         2.14006616 2.14006616 2.14006616]\n",
      " [1.73460106 1.73460106 2.14006616 0.         1.73460106 1.73460106]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 0.         2.83321334]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 2.83321334 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def process_corpus_tags(corpus):\n",
    "    unique_tags = set()\n",
    "    tag_count = Counter()\n",
    "    tag_pair_count = defaultdict(int)\n",
    "    total_tags = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        line = clean_str(line)\n",
    "        # get pos tags for words in the query\n",
    "        tags = [one[1].lower() for one in nltk.pos_tag(nltk.word_tokenize(line))]\n",
    "        if '' in tags:\n",
    "            print(line)\n",
    "        tags = line.split()\n",
    "        total_tags += len(tags)\n",
    "        tag_count.update(tags)\n",
    "        for i, tag in enumerate(tags):\n",
    "            unique_tags.add(tag)\n",
    "            for j in range(i + 1, len(tags)):\n",
    "                pair = tuple(sorted([tag, tags[j]]))\n",
    "                tag_pair_count[pair] += 1\n",
    "    \n",
    "    tag_prob = {tag: count / total_tags for tag, count in tag_count.items()}\n",
    "    pair_prob = {pair: count / total_tags for pair, count in tag_pair_count.items()}\n",
    "    \n",
    "    return tag_prob, tag_pair_prob, unique_tags\n",
    "\n",
    "def calculate_pmi(word_prob, pair_prob, word1, word2):\n",
    "    pair = tuple(sorted([word1, word2]))\n",
    "    if pair in pair_prob and word1 in word_prob and word2 in word_prob:\n",
    "        pmi = log(pair_prob[pair] / (word_prob[word1] * word_prob[word2]))\n",
    "        return pmi\n",
    "    return 0.0\n",
    "\n",
    "def create_pmi_matrix(sentence, tag_prob, tag_pair_prob, tag_index):\n",
    "    words = clean_str(sentence).split()\n",
    "    n = len(words)\n",
    "    pmi_matrix = np.zeros((n, n))\n",
    "    node_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            node_list.append(word_index[word])\n",
    "        else:\n",
    "            node_list.append(-1)\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pmi = calculate_pmi(word_prob, pair_prob, words[i], words[j])\n",
    "            pmi_matrix[i, j] = pmi\n",
    "            pmi_matrix[j, i] = pmi  # PMI matrix is symmetric\n",
    "    \n",
    "    return pmi_matrix, node_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"Another line; with more: punctuation.\",\n",
    "    \"Is this working? Yes, it is!\"\n",
    "]\n",
    "\n",
    "tag_prob, tag_pair_prob, unique_tags = process_corpus_tags(corpus)\n",
    "tag_index = {tag: index for index, tag in enumerate(sorted(unique_tags))}\n",
    "\n",
    "sentence = \"Hello world, this is a test\"\n",
    "pmi_matrix, node_list = create_pmi_matrix(sentence, word_prob, pair_prob, word_index)\n",
    "print(\"PMI Adjacency Matrix:\")\n",
    "print(pmi_matrix)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Ensure you have the necessary nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'NOUN'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
      "Phrases: ['dog', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words, tagset='universal')\n",
    "    return pos_tags\n",
    "\n",
    "def identify_phrases(pos_tags, np_pattern, vp_pattern):\n",
    "    pos_sequence = ' '.join([tag for word, tag in pos_tags])\n",
    "    \n",
    "    np_regex = re.compile(np_pattern)\n",
    "    vp_regex = re.compile(vp_pattern)\n",
    "    \n",
    "    np_matches = [(match.start(), match.end()) for match in np_regex.finditer(pos_sequence)]\n",
    "    vp_matches = [(match.start(), match.end()) for match in vp_regex.finditer(pos_sequence)]\n",
    "    \n",
    "    phrases = []\n",
    "    for start, end in sorted(np_matches + vp_matches):\n",
    "        phrase = ' '.join([word for word, tag in pos_tags[start:end]])\n",
    "        phrases.append(phrase)\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "# Define the patterns\n",
    "np_pattern = r\"((DET)?(NUM)*((ADJ)(PUNCT)?(CONJ)?)*(((NOUN)|(PROPN))(PART)?)+)\"\n",
    "vp_pattern = r\"((AUX)*(ADV)*(VERB))\"\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "pos_tags = pos_tag_sentence(sentence)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "phrases = identify_phrases(pos_tags, np_pattern, vp_pattern)\n",
    "print(\"Phrases:\", phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP with spacy model (using this for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox\n",
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "sample_text = 'The quick brown fox jumps over the lazy dog'\n",
    "sample_doc = nlp(sample_text)\n",
    "# Extract Noun Phrases\n",
    "for chunk in sample_doc.noun_chunks:\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP with spacy model (obtained from fyp student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox jumps\n",
      "jumps over\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "sample_text = ('The quick brown fox jumps over the lazy dog')\n",
    "expression = r'(<VERB>?<ADV>*<VERB>+)'\n",
    "# pattern = [{\"TEXT\": {\"REGEX\": '(<VERB>?<ADV>*<VERB>+)'}}]\n",
    "patterns = [\n",
    "    [{\"POS\": \"ADV\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"PRON\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}, {\"POS\": \"ADP\"}]\n",
    "]\n",
    "\n",
    "\n",
    "# get_verb_phrases = textacy.extract.token_matches(sample_text, patterns=patterns)\n",
    "# verb_phrases = []\n",
    "# for verb_phrase in get_verb_phrases:\n",
    "#     verb_phrases.append(verb_phrase)\n",
    "sample_doc = textacy.make_spacy_doc(sample_text,\n",
    "                                        lang='en_core_web_lg')\n",
    "verb_phrases = textacy.extract.token_matches(sample_doc, patterns)\n",
    "# Print all Verb Phrase\n",
    "for chunk in verb_phrases:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in re.finditer(expression, sample_doc.text):\n",
    "    start, end = match.span()\n",
    "    span = sample_doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP with spacy using grammar tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb Phrases: ['He is eating an apple while she reads a book .', 'while she reads a book']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_verb_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    verb_phrases = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb_phrase = ' '.join([child.text for child in token.subtree])\n",
    "            verb_phrases.append(verb_phrase)\n",
    "    return verb_phrases\n",
    "\n",
    "sentence = \"He is eating an apple while she reads a book.\"\n",
    "verb_phrases = extract_verb_phrases(sentence)\n",
    "print(\"Verb Phrases:\", verb_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP using nltk regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb Phrases: ['is eating an apple while she reads a book .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_verb_phrases(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    chunk_grammar = \"VP: {<VB.*><.*>*}\"\n",
    "    chunk_parser = RegexpParser(chunk_grammar)\n",
    "    tree = chunk_parser.parse(tagged)\n",
    "\n",
    "    verb_phrases = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"VP\":\n",
    "            verb_phrase = ' '.join(word for word, pos in subtree.leaves())\n",
    "            verb_phrases.append(verb_phrase)\n",
    "    return verb_phrases\n",
    "\n",
    "sentence = \"He is eating an apple while she reads a book.\"\n",
    "verb_phrases = extract_verb_phrases(sentence)\n",
    "print(\"Verb Phrases:\", verb_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined phrase (NP and VP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "sample_text = 'The quick brown fox jumps over the lazy dog'\n",
    "sample_doc = nlp(sample_text)\n",
    "# Extract Noun Phrases\n",
    "for chunk in sample_doc.noun_chunks:\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jylee/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "phrase_list = [ 'play an active role', 'participate actively', 'active lifestyle']\n",
    "\n",
    "model = SentenceTransformer('whaleloops/phrase-bert')\n",
    "phrase_embs = model.encode( phrase_list )\n",
    "[p1, p2, p3] = phrase_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Adjacency Matrix:\n",
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_dependency_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dependencies = [(token.text, token.head.text, token.dep_) for token in doc]\n",
    "    return dependencies, [token.text for token in doc]\n",
    "\n",
    "def create_adjacency_matrix(sentence):\n",
    "    dependencies, words = get_dependency_parse(sentence)\n",
    "    word_index = {word: i for i, word in enumerate(words)}\n",
    "    n = len(words)\n",
    "    \n",
    "    adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for word, head, dep in dependencies:\n",
    "        if word != head:  # Skip self-loops\n",
    "            adjacency_matrix[word_index[head]][word_index[word]] = 1\n",
    "    \n",
    "    return adjacency_matrix, words\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "adj_matrix, words = create_adjacency_matrix(sentence)\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adj_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent shape (1, 100)\n",
      "entity_emb_cos 0.9999999999999999\n",
      "ent 1\n",
      "entities ['george washington']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "ent2id_new = json.load(open('./pretrained_emb/NELL_KG/ent2ids_refined', 'r'))        \n",
    "ent_mapping = {} \n",
    "entity_set = set()\n",
    "adj_ent_index = []\n",
    "\n",
    "def get_adj_ent_index(query, ent_mapping, ent2id_new):\n",
    "    # named entity recognition\n",
    "    np_list = []\n",
    "    ent_list = []\n",
    "    index = []\n",
    "    \n",
    "    # extract NP first\n",
    "    doc = nlp(query)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        np_list.append(chunk.text)\n",
    "    \n",
    "    # for every word in the NER dictionary\n",
    "    for key in ent2id_new.keys(): \n",
    "        # check if the word is in the text\n",
    "        if key in np_list: \n",
    "            # check if word is already in the mapping dict\n",
    "            if key not in ent_mapping: \n",
    "                # add word to ent_list\n",
    "                ent_list.append(key)\n",
    "                # add word to mapping dict as word:idx_in_ent_list\n",
    "                ent_mapping[key] = len(ent_mapping)\n",
    "                # update the set\n",
    "                entity_set.update(ent_list)\n",
    "            if ent_mapping[key] not in index: \n",
    "                index.append(ent_mapping[key])\n",
    "    # entity adjacency (index) matrix: list[list] of entities present in the sentences\n",
    "    adj_ent_index.append(index)\n",
    "\n",
    "\n",
    "sample_query = ['the quick brown fox jumps over the lazy dog', 'who is george washington']\n",
    "for sent in sample_query:\n",
    "    get_adj_ent_index(sent, ent_mapping, ent2id_new)\n",
    "# json.dump([adj_ent_index, ent_mapping],\n",
    "#           open('./{}_data/index_and_mapping.json'.format(dataset_name), 'w'), ensure_ascii=False)\n",
    "ent_emb = []\n",
    "TransE_emb_file = np.loadtxt('./pretrained_emb/NELL_KG/entity2vec.TransE')\n",
    "TransE_emb = []\n",
    "\n",
    "for i in range(len(TransE_emb_file)):\n",
    "    TransE_emb.append(list(TransE_emb_file[i, :]))\n",
    "\n",
    "rows = []\n",
    "data = []\n",
    "columns = []\n",
    "\n",
    "max_num = len(ent_mapping)\n",
    "# creating a coo format for matrix of adj_ent_index\n",
    "for sent_i, indices in enumerate(adj_ent_index):\n",
    "    for index in indices:\n",
    "        data.append(1)\n",
    "        rows.append(sent_i)\n",
    "        columns.append(index)\n",
    "\n",
    "# create a matrice of ones and zeros\n",
    "# ones correspond to (sentence_index, entity_index) i.e. which entities are present in the sentence\n",
    "adj_ent = coo_matrix((data, (rows, columns)), shape=(len(adj_ent_index), max_num))\n",
    "# for entity in entity mapping\n",
    "for key in ent_mapping.keys():\n",
    "    # add embedding to ent_emb\n",
    "    ent_emb.append(TransE_emb[ent2id_new[key]])\n",
    "\n",
    "ent_emb = np.array(ent_emb)\n",
    "print('ent shape', ent_emb.shape)\n",
    "ent_emb_normed = ent_emb / np.sqrt(np.square(ent_emb).sum(-1, keepdims=True))\n",
    "adj_emb = np.matmul(ent_emb_normed, ent_emb_normed.transpose())\n",
    "print('entity_emb_cos', np.mean(np.mean(adj_emb, -1)))\n",
    "# pkl.dump(np.array(ent_emb), open('./{}_data/entity_emb.pkl'.format(dataset_name), 'wb'))\n",
    "# pkl.dump(adj_ent, open('./{}_data/adj_query2entity.pkl'.format(dataset_name), 'wb'))\n",
    "\n",
    "entity_nodes = list(entity_set)\n",
    "\n",
    "print('ent', len(entity_nodes))\n",
    "print('entities', entity_nodes)\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent shape (6, 100)\n",
      "entity_emb_cos 0.18653110780888263\n",
      "ent 6\n",
      "entities ['fox', 'jump', 'row', 'quick', 'dog', 'brown']\n"
     ]
    }
   ],
   "source": [
    "sample_query = 'the quick brown fox jumps over the lazy dog'\n",
    "get_adj_ent_index(sample_query, ent_mapping, ent2id_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issue for data\n",
    "- structure of node embeddings  (num_nodes, emb_size)???\n",
    "    - PMI: for word nodes: use glove\n",
    "    - Entity: use TransE\n",
    "    - Dependency parsing??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data for demonstration\n",
    "def generate_random_graphs(num_nodes, num_node_features, num_classes, num_graphs):\n",
    "    x = torch.rand((num_nodes, num_node_features), dtype=torch.float)  # Node features\n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2), dtype=torch.long)  # Edges\n",
    "    y = torch.randint(0, num_classes, (num_nodes,), dtype=torch.long)  # Node labels\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create datasets for 5 separate graphs\n",
    "datasets = [generate_random_graphs(num_nodes=10, num_node_features=5, num_classes=3) for _ in range(100)]\n",
    "\n",
    "dataloaders = [DataLoader(dataset, batch_size=16, shuffle=True) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "# Define a single GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "# Final model combining multiple GCNs and a classification layer\n",
    "class MultiGCNClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_gncs, num_classes):\n",
    "        super(MultiGCNClassifier, self).__init__()\n",
    "        self.gcns = torch.nn.ModuleList([GCN(in_channels, hidden_channels, out_channels) for _ in range(num_gncs)])\n",
    "        self.linear = torch.nn.Linear(out_channels * num_gncs, num_classes)\n",
    "\n",
    "    def forward(self, graphs):\n",
    "        embeddings = []\n",
    "        for i, graph in enumerate(graphs):\n",
    "            x, edge_index = graph.x, graph.edge_index\n",
    "            embeddings.append(self.gcns[i](x, edge_index))\n",
    "        \n",
    "        # Concatenate the embeddings from each GCN\n",
    "        concatenated = torch.cat(embeddings, dim=1)\n",
    "        out = self.linear(concatenated)\n",
    "        return F.log_softmax(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, optimizer, and loss function\n",
    "num_gncs = 5\n",
    "model = MultiGCNClassifier(in_channels=5, hidden_channels=16, out_channels=16, num_gncs=num_gncs, num_classes=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0893805027008057\n",
      "Epoch 2, Loss: 1.1288957595825195\n",
      "Epoch 3, Loss: 1.1174507141113281\n",
      "Epoch 4, Loss: 1.0951950550079346\n",
      "Epoch 5, Loss: 1.0923782587051392\n",
      "Epoch 6, Loss: 1.0871156454086304\n",
      "Epoch 7, Loss: 1.090480089187622\n",
      "Epoch 8, Loss: 1.11361563205719\n",
      "Epoch 9, Loss: 1.0942275524139404\n",
      "Epoch 10, Loss: 1.1013057231903076\n",
      "Epoch 11, Loss: 1.0750558376312256\n",
      "Epoch 12, Loss: 1.0790274143218994\n",
      "Epoch 13, Loss: 1.1071563959121704\n",
      "Epoch 14, Loss: 1.0836830139160156\n",
      "Epoch 15, Loss: 1.0988441705703735\n",
      "Epoch 16, Loss: 1.1076276302337646\n",
      "Epoch 17, Loss: 1.1086926460266113\n",
      "Epoch 18, Loss: 1.1190255880355835\n",
      "Epoch 19, Loss: 1.0933531522750854\n",
      "Epoch 20, Loss: 1.0927650928497314\n",
      "Accuracy: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for batches in zip(*dataloaders):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process each batch through the respective GCN\n",
    "        out = model(batches)\n",
    "        \n",
    "        # Use the labels for first graph of the batch for loss calculation\n",
    "        loss = F.nll_loss(out, batches[0].y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for batches in zip(*dataloaders):\n",
    "    out = model(batches)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct += (pred == batches[0].y).sum().item()\n",
    "    total += batches[0].num_nodes\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".shine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
