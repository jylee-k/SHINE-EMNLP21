{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LEE JUNYOUNG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocess/trec_split.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_data = data['train']\n",
    "test_data = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'How did serfdom develop in and then leave Russia ?',\n",
       " 'coarse_label': 2,\n",
       " 'fine_label': 26}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to form any kinds of graph, you first need to go through the entire corpus and obtain {idx:node}. we first do this for word, then pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for cleaning text\n",
    "import string, re\n",
    "\n",
    "def expand_contractions(text):\n",
    "    # Define a list of patterns and their replacements\n",
    "    patterns = [\n",
    "        (r\"n't\\b\", \" not\"),\n",
    "        (r\"'re\\b\", \" are\"),\n",
    "        (r\"'ll\\b\", \" will\"),\n",
    "        (r\"'ve\\b\", \" have\"),\n",
    "        (r\"'m\\b\", \" am\"),\n",
    "        (r\"'d\\b\", \" would\"),\n",
    "        (r\"'s\\b\", \" is\"),\n",
    "        (r\"\\bcan't\\b\", \"cannot\"),\n",
    "        (r\"\\bshan't\\b\", \"shall not\"),\n",
    "        (r\"\\bwon't\\b\", \"will not\"),\n",
    "        (r\"\\blet's\\b\", \"let us\"),\n",
    "    ]\n",
    "    \n",
    "    # Apply each pattern and replacement in turn\n",
    "    for pattern, replacement in patterns:\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_str(sentence ,use=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    if not use: return sentence\n",
    "    \n",
    "    sentence = expand_contractions(sentence)\n",
    "\n",
    "    sentence = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" \\'s\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" \\'ve\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" n\\'t\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" \\'re\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" \\'d\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" \\'ll\", sentence)\n",
    "    sentence = re.sub(r\",\", \" , \", sentence)\n",
    "    sentence = re.sub(r\"!\", \" ! \", sentence)\n",
    "    sentence = re.sub(r\"\\(\", \" \\( \", sentence)\n",
    "    sentence = re.sub(r\"\\)\", \" \\) \", sentence)\n",
    "    sentence = re.sub(r\"\\?\", \" \\? \", sentence)\n",
    "    sentence = re.sub(r\"\\s{2,}\", \" \", sentence)\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return sentence.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI Adjacency Matrix:\n",
      "[[0.         2.83321334 2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.83321334 0.         2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.14006616 2.14006616 0.         2.14006616 2.14006616 2.14006616]\n",
      " [1.73460106 1.73460106 2.14006616 0.         1.73460106 1.73460106]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 0.         2.83321334]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 2.83321334 0.        ]]\n",
      "[2, 12, 9, 3, 0, 8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    unique_words = set()\n",
    "    word_count = Counter()\n",
    "    pair_count = defaultdict(int)\n",
    "    total_words = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        line = clean_str(line)\n",
    "        words = line.split()\n",
    "        total_words += len(words)\n",
    "        word_count.update(words)\n",
    "        for i, word in enumerate(words):\n",
    "            unique_words.add(word)\n",
    "            for j in range(i + 1, len(words)):\n",
    "                pair = tuple(sorted([word, words[j]]))\n",
    "                pair_count[pair] += 1\n",
    "    \n",
    "    word_prob = {word: count / total_words for word, count in word_count.items()}\n",
    "    pair_prob = {pair: count / total_words for pair, count in pair_count.items()}\n",
    "    \n",
    "    return word_prob, pair_prob, unique_words\n",
    "\n",
    "def calculate_pmi(word_prob, pair_prob, word1, word2):\n",
    "    pair = tuple(sorted([word1, word2]))\n",
    "    if pair in pair_prob and word1 in word_prob and word2 in word_prob:\n",
    "        pmi = log(pair_prob[pair] / (word_prob[word1] * word_prob[word2]))\n",
    "        return pmi\n",
    "    return 0.0\n",
    "\n",
    "def create_pmi_matrix(sentence, word_prob, pair_prob, word_index):\n",
    "    words = clean_str(sentence).split()\n",
    "    n = len(words)\n",
    "    pmi_matrix = np.zeros((n, n))\n",
    "    node_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            node_list.append(word_index[word])\n",
    "        else:\n",
    "            node_list.append(-1)\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pmi = calculate_pmi(word_prob, pair_prob, words[i], words[j])\n",
    "            pmi_matrix[i, j] = pmi\n",
    "            pmi_matrix[j, i] = pmi  # PMI matrix is symmetric\n",
    "    \n",
    "    return pmi_matrix, node_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"Another line; with more: punctuation.\",\n",
    "    \"Is this working? Yes, it is!\"\n",
    "]\n",
    "\n",
    "word_prob, pair_prob, unique_words = process_corpus(corpus)\n",
    "word_index = {word: index for index, word in enumerate(sorted(unique_words))}\n",
    "\n",
    "sentence = \"Hello world, this is a test\"\n",
    "pmi_matrix, node_list = create_pmi_matrix(sentence, word_prob, pair_prob, word_index)\n",
    "print(\"PMI Adjacency Matrix:\")\n",
    "print(pmi_matrix)\n",
    "print(node_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus_tags(corpus: list[list]):\n",
    "    unique_pos_tags = set()\n",
    "    pos_tag_count = Counter()\n",
    "    pos_tag_pair_count = defaultdict(int)\n",
    "    total_pos_tags = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        total_pos_tags += len(line)\n",
    "        pos_tag_count.update(line)\n",
    "        \n",
    "        for i, pos_tag in enumerate(line):\n",
    "            unique_pos_tags.add(pos_tag)\n",
    "            \n",
    "            for j in range(len(line)):\n",
    "                if i == j: continue\n",
    "                pair = tuple(sorted([pos_tag, line[j]]))\n",
    "                pos_tag_count[pair] += 1\n",
    "    \n",
    "    pos_tag_prob = {pos_tag: count / total_pos_tags for pos_tag, count in pos_tag_count.items()}\n",
    "    pos_tag_pair_prob = {pos_tag_pair: count / total_pos_tags for pos_tag_pair, count in pos_tag_pair_count.items()}\n",
    "    \n",
    "    return pos_tag_prob, pos_tag_pair_prob, unique_pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### euclidean distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between the vectors is: 5.196152422706632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two embedding vectors.\n",
    "    \n",
    "    Args:\n",
    "    vector1 (np.array): First embedding vector.\n",
    "    vector2 (np.array): Second embedding vector.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Euclidean distance between the two vectors.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "# Example usage\n",
    "embedding1 = np.array([1, 2, 3])\n",
    "embedding2 = np.array([4, 5, 6])\n",
    "\n",
    "distance = euclidean_distance(embedding1, embedding2)\n",
    "print(f\"The Euclidean distance between the vectors is: {distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Ensure you have the necessary nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'NOUN'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
      "Phrases: ['dog', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words, tagset='universal')\n",
    "    return pos_tags\n",
    "\n",
    "def identify_phrases(pos_tags, np_pattern, vp_pattern):\n",
    "    pos_sequence = ' '.join([tag for word, tag in pos_tags])\n",
    "    \n",
    "    np_regex = re.compile(np_pattern)\n",
    "    vp_regex = re.compile(vp_pattern)\n",
    "    \n",
    "    np_matches = [(match.start(), match.end()) for match in np_regex.finditer(pos_sequence)]\n",
    "    vp_matches = [(match.start(), match.end()) for match in vp_regex.finditer(pos_sequence)]\n",
    "    \n",
    "    phrases = []\n",
    "    for start, end in sorted(np_matches + vp_matches):\n",
    "        phrase = ' '.join([word for word, tag in pos_tags[start:end]])\n",
    "        phrases.append(phrase)\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "# Define the patterns\n",
    "np_pattern = r\"((DET)?(NUM)*((ADJ)(PUNCT)?(CONJ)?)*(((NOUN)|(PROPN))(PART)?)+)\"\n",
    "vp_pattern = r\"((AUX)*(ADV)*(VERB))\"\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "pos_tags = pos_tag_sentence(sentence)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "phrases = identify_phrases(pos_tags, np_pattern, vp_pattern)\n",
    "print(\"Phrases:\", phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP with spacy model (using this for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox\n",
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "sample_text = 'The quick brown fox jumps over the lazy dog'\n",
    "sample_doc = nlp(sample_text)\n",
    "# Extract Noun Phrases\n",
    "for chunk in sample_doc.noun_chunks:\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP with spacy model (obtained from fyp student) (using this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox jumps\n",
      "jumps over\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "sample_text = ('The quick brown fox jumps over the lazy dog')\n",
    "expression = r'(<VERB>?<ADV>*<VERB>+)'\n",
    "# pattern = [{\"TEXT\": {\"REGEX\": '(<VERB>?<ADV>*<VERB>+)'}}]\n",
    "vp_patterns = [\n",
    "    [{\"POS\": \"ADV\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"PRON\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}, {\"POS\": \"ADP\"}]\n",
    "]\n",
    "\n",
    "\n",
    "# get_verb_phrases = textacy.extract.token_matches(sample_text, patterns=patterns)\n",
    "# verb_phrases = []\n",
    "# for verb_phrase in get_verb_phrases:\n",
    "#     verb_phrases.append(verb_phrase)\n",
    "sample_doc = textacy.make_spacy_doc(sample_text,\n",
    "                                        lang='en_core_web_lg')\n",
    "verb_phrases = textacy.extract.token_matches(sample_doc, vp_patterns)\n",
    "# Print all Verb Phrase\n",
    "for chunk in verb_phrases:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in re.finditer(expression, sample_doc.text):\n",
    "    start, end = match.span()\n",
    "    span = sample_doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP with spacy using grammar tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb Phrases: ['He is eating an apple while she reads a book .', 'while she reads a book']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_verb_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    verb_phrases = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb_phrase = ' '.join([child.text for child in token.subtree])\n",
    "            verb_phrases.append(verb_phrase)\n",
    "    return verb_phrases\n",
    "\n",
    "sentence = \"He is eating an apple while she reads a book.\"\n",
    "verb_phrases = extract_verb_phrases(sentence)\n",
    "print(\"Verb Phrases:\", verb_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP using nltk regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m verb_phrases\n\u001b[0;32m     15\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHe is eating an apple while she reads a book.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m verb_phrases \u001b[38;5;241m=\u001b[39m \u001b[43mextract_verb_phrases\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerb Phrases:\u001b[39m\u001b[38;5;124m\"\u001b[39m, verb_phrases)\n",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m, in \u001b[0;36mextract_verb_phrases\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_verb_phrases\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(text)\n\u001b[0;32m      3\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m pos_tag(words)\n\u001b[0;32m      4\u001b[0m     chunk_grammar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVP: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<VB.*><.*>*}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_verb_phrases(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    chunk_grammar = \"VP: {<VB.*><.*>*}\"\n",
    "    chunk_parser = RegexpParser(chunk_grammar)\n",
    "    tree = chunk_parser.parse(tagged)\n",
    "\n",
    "    verb_phrases = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"VP\":\n",
    "            verb_phrase = ' '.join(word for word, pos in subtree.leaves())\n",
    "            verb_phrases.append(verb_phrase)\n",
    "    return verb_phrases\n",
    "\n",
    "sentence = \"He is eating an apple while she reads a book.\"\n",
    "verb_phrases = extract_verb_phrases(sentence)\n",
    "print(\"Verb Phrases:\", verb_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined phrase (NP and VP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_patterns = [\n",
    "    [{\"POS\": \"ADV\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"PRON\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}, {\"POS\": \"ADP\"}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases:  [the Euclidean distance, two embedding vectors, Calculate the Euclidean, embedding vectors]\n",
      "POS Tags:  ['DET ADJ NOUN', 'NUM VERB NOUN', 'VERB DET ADJ', 'VERB NOUN']\n"
     ]
    }
   ],
   "source": [
    "import spacy, textacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "sample_text = 'Calculate the Euclidean distance between two embedding vectors.'\n",
    "sample_doc = nlp(sample_text)\n",
    "verb_phrases = textacy.extract.token_matches(sample_doc, vp_patterns)\n",
    "\n",
    "phrase_list = []\n",
    "tag_list = []\n",
    "\n",
    "# Extract Noun Phrases and corresponding pos tags\n",
    "for chunk in sample_doc.noun_chunks:\n",
    "    phrase_list.append(chunk)\n",
    "    tag_list.append(' '.join([t.pos_ for t in chunk]))\n",
    "# Print all Verb Phrase and corresponding pos tags\n",
    "for chunk in verb_phrases:\n",
    "    phrase_list.append(chunk)\n",
    "    tag_list.append(' '.join([t.pos_ for t in chunk]))\n",
    "\n",
    "print(\"Phrases: \", phrase_list)\n",
    "print(\"POS Tags: \", tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding vectors for pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'NNP': 0, 'DT': 1, 'VBG': 2, 'NN': 3, 'VB': 4, 'VBZ': 5}\n",
      "One-hot encoded vectors:\n",
      "[1. 0. 0. 1. 1. 0.]\n",
      "[0. 1. 1. 1. 0. 1.]\n",
      "[0. 0. 0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example corpus of sentences transformed into lists of POS tags\n",
    "corpus = [\n",
    "    ['VB', 'NN', 'NNP'],\n",
    "    ['DT', 'NN', 'VBZ', 'VBG'],\n",
    "    ['NN', 'NN', 'VB'],\n",
    "]\n",
    "\n",
    "# Step 1: Collect all unique POS tags to create a vocabulary\n",
    "all_pos_tags = set(tag for sentence in corpus for tag in sentence)\n",
    "\n",
    "# Step 2: Create a mapping from each POS tag to a unique index\n",
    "pos_to_index = {tag: idx for idx, tag in enumerate(all_pos_tags)}\n",
    "\n",
    "# Step 3: Generate one-hot encoded vectors\n",
    "def one_hot_encode(pos_list, pos_to_index):\n",
    "    vector = np.zeros(len(pos_to_index))\n",
    "    for pos in pos_list:\n",
    "        if pos in pos_to_index:\n",
    "            vector[pos_to_index[pos]] = 1\n",
    "    return vector\n",
    "\n",
    "# Example usage\n",
    "encoded_corpus = [one_hot_encode(sentence, pos_to_index) for sentence in corpus]\n",
    "\n",
    "print(\"Vocabulary:\", pos_to_index)\n",
    "print(\"One-hot encoded vectors:\")\n",
    "for vec in encoded_corpus:\n",
    "    print(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jylee/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "phrase_list = ['play an active role', 'participate actively', 'active lifestyle']\n",
    "\n",
    "model = SentenceTransformer('whaleloops/phrase-bert')\n",
    "phrase_embs = model.encode(phrase_list)\n",
    "[p1, p2, p3] = phrase_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Adjacency Matrix:\n",
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_dependency_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dependencies = [(token.text, token.head.text, token.dep_) for token in doc]\n",
    "    return dependencies, [token.text for token in doc]\n",
    "\n",
    "def create_adjacency_matrix(sentence):\n",
    "    dependencies, words = get_dependency_parse(sentence)\n",
    "    word_index = {word: i for i, word in enumerate(words)}\n",
    "    n = len(words)\n",
    "    \n",
    "    adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for word, head, dep in dependencies:\n",
    "        if word != head:  # Skip self-loops\n",
    "            adjacency_matrix[word_index[head]][word_index[word]] = 1\n",
    "    \n",
    "    return adjacency_matrix, words\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "adj_matrix, words = create_adjacency_matrix(sentence)\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adj_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent shape (1, 100)\n",
      "entity_emb_cos 0.9999999999999999\n",
      "ent 1\n",
      "entities ['george washington']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "ent2id_new = json.load(open('./pretrained_emb/NELL_KG/ent2ids_refined', 'r'))        \n",
    "ent_mapping = {} \n",
    "entity_set = set()\n",
    "adj_ent_index = []\n",
    "\n",
    "def get_adj_ent_index(query, ent_mapping, ent2id_new):\n",
    "    # named entity recognition\n",
    "    np_list = []\n",
    "    ent_list = []\n",
    "    index = []\n",
    "    \n",
    "    # extract NP first\n",
    "    doc = nlp(query)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        np_list.append(chunk.text)\n",
    "    \n",
    "    # for every word in the NER dictionary\n",
    "    for key in ent2id_new.keys(): \n",
    "        # check if the word is in the text\n",
    "        if key in np_list: \n",
    "            # check if word is already in the mapping dict\n",
    "            if key not in ent_mapping: \n",
    "                # add word to ent_list\n",
    "                ent_list.append(key)\n",
    "                # add word to mapping dict as word:idx_in_ent_list\n",
    "                ent_mapping[key] = len(ent_mapping)\n",
    "                # update the set\n",
    "                entity_set.update(ent_list)\n",
    "            if ent_mapping[key] not in index: \n",
    "                index.append(ent_mapping[key])\n",
    "    # entity adjacency (index) matrix: list[list] of entities present in the sentences\n",
    "    adj_ent_index.append(index)\n",
    "\n",
    "\n",
    "sample_query = ['the quick brown fox jumps over the lazy dog', 'who is george washington']\n",
    "for sent in sample_query:\n",
    "    get_adj_ent_index(sent, ent_mapping, ent2id_new)\n",
    "# json.dump([adj_ent_index, ent_mapping],\n",
    "#           open('./{}_data/index_and_mapping.json'.format(dataset_name), 'w'), ensure_ascii=False)\n",
    "ent_emb = []\n",
    "TransE_emb_file = np.loadtxt('./pretrained_emb/NELL_KG/entity2vec.TransE')\n",
    "TransE_emb = []\n",
    "\n",
    "for i in range(len(TransE_emb_file)):\n",
    "    TransE_emb.append(list(TransE_emb_file[i, :]))\n",
    "\n",
    "rows = []\n",
    "data = []\n",
    "columns = []\n",
    "\n",
    "max_num = len(ent_mapping)\n",
    "# creating a coo format for matrix of adj_ent_index\n",
    "for sent_i, indices in enumerate(adj_ent_index):\n",
    "    for index in indices:\n",
    "        data.append(1)\n",
    "        rows.append(sent_i)\n",
    "        columns.append(index)\n",
    "\n",
    "# create a matrice of ones and zeros\n",
    "# ones correspond to (sentence_index, entity_index) i.e. which entities are present in the sentence\n",
    "adj_ent = coo_matrix((data, (rows, columns)), shape=(len(adj_ent_index), max_num))\n",
    "# for entity in entity mapping\n",
    "for key in ent_mapping.keys():\n",
    "    # add embedding to ent_emb\n",
    "    ent_emb.append(TransE_emb[ent2id_new[key]])\n",
    "\n",
    "ent_emb = np.array(ent_emb)\n",
    "print('ent shape', ent_emb.shape)\n",
    "ent_emb_normed = ent_emb / np.sqrt(np.square(ent_emb).sum(-1, keepdims=True))\n",
    "adj_emb = np.matmul(ent_emb_normed, ent_emb_normed.transpose())\n",
    "print('entity_emb_cos', np.mean(np.mean(adj_emb, -1)))\n",
    "# pkl.dump(np.array(ent_emb), open('./{}_data/entity_emb.pkl'.format(dataset_name), 'wb'))\n",
    "# pkl.dump(adj_ent, open('./{}_data/adj_query2entity.pkl'.format(dataset_name), 'wb'))\n",
    "\n",
    "entity_nodes = list(entity_set)\n",
    "\n",
    "print('ent', len(entity_nodes))\n",
    "print('entities', entity_nodes)\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent shape (6, 100)\n",
      "entity_emb_cos 0.18653110780888263\n",
      "ent 6\n",
      "entities ['fox', 'jump', 'row', 'quick', 'dog', 'brown']\n"
     ]
    }
   ],
   "source": [
    "sample_query = 'the quick brown fox jumps over the lazy dog'\n",
    "get_adj_ent_index(sample_query, ent_mapping, ent2id_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "dataset_name = 'bloom'\n",
    "with open(f'preprocess/{dataset_name}_split.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.DataFrame(data['train']).transpose().reset_index(drop=True)\n",
    "test_data = pd.DataFrame(data['test']).transpose().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyze the product-market options that are av...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Define attribution.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Given three possible approaches to implement t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Predict what will happen next in.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Define “ecosystem services” and describe how t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>Compare and contrast preview questions, clarif...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>Name the amino acids for the following single ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>List and explain THREE factors that are affect...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>After designing an experiment, examining the r...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>Explain the purposes of scaling.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2017 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Analyze the product-market options that are av...     0\n",
       "1                                  Define attribution.      4\n",
       "2     Given three possible approaches to implement t...     3\n",
       "3                     Predict what will happen next in.     3\n",
       "4     Define “ecosystem services” and describe how t...     2\n",
       "...                                                 ...   ...\n",
       "2012  Compare and contrast preview questions, clarif...     0\n",
       "2013  Name the amino acids for the following single ...     4\n",
       "2014  List and explain THREE factors that are affect...     2\n",
       "2015  After designing an experiment, examining the r...     3\n",
       "2016                  Explain the purposes of scaling.      2\n",
       "\n",
       "[2017 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_dependency_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dependencies = [(token.text, token.head.text, token.dep_) for token in doc]\n",
    "    # return dependencies, [token.text for token in doc]\n",
    "    return dependencies\n",
    "\n",
    "def create_adjacency_matrix(sentence):\n",
    "    # dependencies, words = get_dependency_parse(sentence)\n",
    "    dependencies = get_dependency_parse(sentence)\n",
    "    word_index = {word: i for i, word in enumerate(words)}\n",
    "    n = len(words)\n",
    "    \n",
    "    adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for word, head, dep in dependencies:\n",
    "        if word != head:  # Skip self-loops\n",
    "            adjacency_matrix[word_index[head]][word_index[word]] = 1\n",
    "    \n",
    "    return adjacency_matrix, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "vp_patterns = [\n",
    "    [{\"POS\": \"ADV\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"PRON\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}, {\"POS\": \"ADP\"}]\n",
    "]\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_phrases(text, return_value = 'phrase'):\n",
    "    doc = nlp(text)\n",
    "    verb_phrases = textacy.extract.token_matches(doc, vp_patterns)\n",
    "\n",
    "    phrase_list = []\n",
    "    tag_list = []\n",
    "\n",
    "    # Extract Noun Phrases and corresponding pos tags\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase_list.append(chunk.text)\n",
    "        tag_list.append(' '.join([t.pos_ for t in chunk]))\n",
    "    # Print all Verb Phrase and corresponding pos tags\n",
    "    for chunk in verb_phrases:\n",
    "        phrase_list.append(chunk.text)\n",
    "        tag_list.append(' '.join([t.pos_ for t in chunk]))\n",
    "    \n",
    "    if return_value == 'phrase':\n",
    "        return phrase_list\n",
    "    elif return_value == 'tag':\n",
    "        return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEE JUNYOUNG\\Desktop\\SHINE-EMNLP21\\.shine\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "import textacy\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import seed_everything\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_tokenized\n",
      "word_embedding generated\n",
      "phrases and phrase_tags generated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data['cleaned_text'] = train_data['text'].apply(lambda x: clean_str(x))\n",
    "text = train_data['cleaned_text'].tolist()\n",
    "test_data['cleaned_text'] = test_data['text'].apply(lambda x: clean_str(x))\n",
    "test_text = test_data['cleaned_text'].tolist()\n",
    "\n",
    "# Tokenize sentences into words\n",
    "train_data['tokenized_text'] = train_data['cleaned_text'].apply(lambda x: x.split())\n",
    "test_data['tokenized_text'] = test_data['cleaned_text'].apply(lambda x: x.split())\n",
    "# labels = train_data['coarse_label'].tolist() # only for trec\n",
    "labels = train_data['label'].tolist()\n",
    "test_labels = test_data['label'].tolist()\n",
    "\n",
    "\n",
    "# 1. Collect all unique words in the corpus\n",
    "tokenized_corpus = train_data['tokenized_text'].tolist()\n",
    "test_tokenized_corpus = test_data['tokenized_text'].tolist()\n",
    "# word_set = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "print('text_tokenized')\n",
    "word_prob, pair_prob, word_set = process_corpus(text)\n",
    "test_word_prob, test_pair_prob, test_word_set = process_corpus(test_text)\n",
    "\n",
    "# 2. Create a mapping from each word to a unique index\n",
    "word_to_index = {word: idx for idx, word in enumerate(word_set)}\n",
    "test_word_to_index = {word: idx for idx, word in enumerate(test_word_set)}\n",
    "\n",
    "# 4. Generate word embeddings using Word2Vec\n",
    "# ps = PorterStemmer()\n",
    "model = Word2Vec(sentences=tokenized_corpus+test_tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "word_embedding = torch.tensor([model.wv[word] for word in word_set])\n",
    "test_word_embedding = torch.tensor([model.wv[word] for word in test_word_set])\n",
    "print('word_embedding generated')\n",
    "\n",
    "# do the same for phrase level\n",
    "train_data['phrases'] = train_data['cleaned_text'].apply(lambda x: extract_phrases(x))\n",
    "test_data['phrases'] = test_data['cleaned_text'].apply(lambda x: extract_phrases(x))\n",
    "train_data['phrase_tags'] = train_data['cleaned_text'].apply(lambda x: extract_phrases(x, 'tag'))\n",
    "test_data['phrase_tags'] = test_data['cleaned_text'].apply(lambda x: extract_phrases(x, 'tag'))\n",
    "\n",
    "phrase_set = set(phrase for phrase_list in train_data['phrases'].tolist() for phrase in phrase_list)\n",
    "test_phrase_set = set(phrase for phrase_list in test_data['phrases'].tolist() for phrase in phrase_list)\n",
    "# phrase_tag_set = set(phrase_tag for phrase_tag_list in train_data['phrase_tags'].tolist() for phrase_tag in phrase_tag_list)\n",
    "\n",
    "phrase_to_index = {phrase: idx for idx, phrase in enumerate(phrase_set)}\n",
    "test_phrase_to_index = {phrase: idx for idx, phrase in enumerate(test_phrase_set)}\n",
    "# phrase_tag_to_index = {phrase_tag: idx for idx, phrase_tag in enumerate(phrase_tag_set)}\n",
    "\n",
    "phrases = train_data['phrases'].tolist()\n",
    "test_phrases = test_data['phrases'].tolist()\n",
    "# phrase_tags = train_data['phrase_tags'].tolist()\n",
    "print('phrases and phrase_tags generated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEE JUNYOUNG\\Desktop\\SHINE-EMNLP21\\.shine\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "phrase_tags = train_data['phrase_tags'].tolist()\n",
    "pos_tag_prob, pos_tag_pair_prob, pos_tag_set = process_corpus_tags(phrase_tags)\n",
    "phrase_tag_to_index = {phrase_tag: idx for idx, phrase_tag in enumerate(pos_tag_set)}\n",
    "\n",
    "test_phrase_tags = test_data['phrase_tags'].tolist()\n",
    "test_pos_tag_prob, test_pos_tag_pair_prob, test_pos_tag_set = process_corpus_tags(test_phrase_tags)\n",
    "test_phrase_tag_to_index = {phrase_tag: idx for idx, phrase_tag in enumerate(test_pos_tag_set)}\n",
    "\n",
    "# Generate phrase embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "phrasebert_model = SentenceTransformer('whaleloops/phrase-bert')\n",
    "phrase_embedding = torch.tensor([phrasebert_model.encode(phrase) for phrase in phrase_set])\n",
    "test_phrase_embedding = torch.tensor([phrasebert_model.encode(phrase) for phrase in test_phrase_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine sim matrix\n",
    "phrase_emb_normed = phrase_embedding / np.sqrt(np.square(phrase_embedding).sum(-1, keepdims=True))\n",
    "cos_sim_matrix = np.matmul(phrase_emb_normed, phrase_emb_normed.transpose(1, 0))\n",
    "\n",
    "test_phrase_emb_normed = test_phrase_embedding / np.sqrt(np.square(test_phrase_embedding).sum(-1, keepdims=True))\n",
    "test_cos_sim_matrix = np.matmul(test_phrase_emb_normed, test_phrase_emb_normed.transpose(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition\n",
    "ent2id_new = json.load(open('./pretrained_emb/NELL_KG/ent2ids_refined', 'r'))\n",
    "TransE_emb_file = np.loadtxt('./pretrained_emb/NELL_KG/entity2vec.TransE')    \n",
    "TransE_emb = []\n",
    "for i in range(len(TransE_emb_file)):\n",
    "    TransE_emb.append(list(TransE_emb_file[i, :]))\n",
    "    \n",
    "def extract_entities(sentence, ent2id_new):\n",
    "    np_list = []\n",
    "    ent_list = []\n",
    "    # extract NP first\n",
    "    doc = nlp(sentence)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        np_list.append(chunk.text)\n",
    "    \n",
    "    for ent in ent2id_new.keys():\n",
    "        if ent in np_list:\n",
    "            ent_list.append(ent)\n",
    "            \n",
    "    return ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entities\n",
    "train_data['named_entities'] = train_data['cleaned_text'].apply(lambda x: extract_entities(x, ent2id_new))\n",
    "named_entities = train_data['named_entities'].tolist()\n",
    "named_entities_set = set(entity for entity_list in named_entities for entity in entity_list)\n",
    "named_entities_to_index = {entity: idx for idx, entity in enumerate(named_entities_set)}\n",
    "\n",
    "test_data['named_entities'] = test_data['cleaned_text'].apply(lambda x: extract_entities(x, ent2id_new))\n",
    "test_named_entities = test_data['named_entities'].tolist()\n",
    "test_named_entities_set = set(entity for entity_list in test_named_entities for entity in entity_list)\n",
    "test_named_entities_to_index = {entity: idx for idx, entity in enumerate(test_named_entities_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_emb = []\n",
    "# for entity in entity mapping\n",
    "for key in named_entities_to_index.keys():\n",
    "    # add transE embedding to ent_emb\n",
    "    ent_emb.append(TransE_emb[ent2id_new[key]])\n",
    "    \n",
    "ent_emb = torch.tensor(ent_emb, dtype=torch.float32)\n",
    "\n",
    "test_ent_emb = []\n",
    "# for entity in entity mapping\n",
    "for key in test_named_entities_to_index.keys():\n",
    "    # add transE embedding to ent_emb\n",
    "    test_ent_emb.append(TransE_emb[ent2id_new[key]])\n",
    "    \n",
    "test_ent_emb = torch.tensor(test_ent_emb, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for pos tags\n",
    "pos_emb = torch.eye(len(phrase_tag_to_index))\n",
    "test_pos_emb = torch.eye(len(test_phrase_tag_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedData(Data):\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'ppmi_edge_index':\n",
    "            return self.ppmi_x.size(0)\n",
    "        if key == 'dp_edge_index':\n",
    "            return self.dp_x.size(0)\n",
    "        if key == 'sem_edge_index':\n",
    "            return self.sem_x.size(0)\n",
    "        if key == 'ner_edge_index':\n",
    "            return self.ner_x.size(0)\n",
    "        if key == 'pos_edge_index':\n",
    "            return self.pos_x.size(0)\n",
    "        return super().__inc__(key, value, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017it [00:08, 247.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Create a list of torch_geometric.data.Data graphs for each sentence in the corpus\n",
    "graphs = []\n",
    "\n",
    "for idx, sentence in tqdm(enumerate(tokenized_corpus)):\n",
    "    \n",
    "    \n",
    "    # PPMI\n",
    "    ppmi_edges = []\n",
    "    ppmi_edge_weights = []\n",
    "    \n",
    "    n = len(sentence)\n",
    "\n",
    "    for i, word1 in enumerate(sentence):\n",
    "        for j in range(i + 1, n):\n",
    "            word2 = sentence[j]\n",
    "            if word1 != word2:\n",
    "                pmi = calculate_pmi(word_prob, pair_prob, word1, word2)\n",
    "                if pmi > 0:\n",
    "                    ppmi_edges.append((word_to_index[word1], word_to_index[word2]))\n",
    "                    ppmi_edge_weights.append(pmi)\n",
    "                    ppmi_edges.append((word_to_index[word2], word_to_index[word1]))\n",
    "                    ppmi_edge_weights.append(pmi) # ppmi is symmetric\n",
    "    \n",
    "    if len(ppmi_edges) == 0:\n",
    "        ppmi_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        ppmi_edge_attr = torch.empty((0,), dtype=torch.float)\n",
    "    else:\n",
    "        ppmi_edge_index = torch.tensor(ppmi_edges, dtype=torch.long).t().contiguous()\n",
    "        ppmi_edge_attr = torch.tensor(ppmi_edge_weights, dtype=torch.float)\n",
    "\n",
    "    \n",
    "    # Dependency Parse\n",
    "    dp_edges = []\n",
    "    \n",
    "    doc = nlp(''.join(sentence))\n",
    "    \n",
    "    dependencies = [(token.text, token.head.text, token.dep_) for token in doc]\n",
    "    # dependencies = get_dependency_parse(text[idx])\n",
    "    for word, head, dep in dependencies:\n",
    "        if word != head and word.strip() and head.strip():  # Skip self-loops and empty tokens\n",
    "            dp_edges.append((word_to_index[head], word_to_index[word]))\n",
    "    \n",
    "    if len(dp_edges) == 0:\n",
    "        dp_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        dp_edge_index = torch.tensor(dp_edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    \n",
    "    # Phrase Embeddings\n",
    "    sem_edges = []\n",
    "    sem_edge_weights = []\n",
    "    \n",
    "    n_phrases = len(phrases[idx])\n",
    "\n",
    "    for i, phrase1 in enumerate(phrases[idx]):\n",
    "        for j in range(n_phrases):\n",
    "            phrase2 = phrases[idx][j]\n",
    "            if phrase1 != phrase2:\n",
    "                sem_edges.append((phrase_to_index[phrase1], phrase_to_index[phrase2]))\n",
    "                sem_edge_weights.append(cos_sim_matrix[phrase_to_index[phrase1], phrase_to_index[phrase2]])\n",
    "                sem_edges.append((phrase_to_index[phrase2], phrase_to_index[phrase1]))\n",
    "                sem_edge_weights.append(cos_sim_matrix[phrase_to_index[phrase2], phrase_to_index[phrase1]])\n",
    "    \n",
    "    if len(sem_edges) == 0:\n",
    "        sem_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        sem_edge_attr = torch.empty((0,), dtype=torch.float)\n",
    "    else:\n",
    "        sem_edge_index = torch.tensor(sem_edges, dtype=torch.long).t().contiguous()\n",
    "        sem_edge_attr = torch.tensor(sem_edge_weights, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    # Phrase tags\n",
    "    \n",
    "    pos_edges = []\n",
    "    pos_edge_weights = []\n",
    "    \n",
    "    n_phrase_tags = len(phrase_tags[idx])\n",
    "    \n",
    "    for i, tag1 in enumerate(phrase_tags[idx]):\n",
    "        for j in range(i + 1, n_phrase_tags):\n",
    "            tag2 = phrase_tags[idx][j]\n",
    "            if tag1 != tag2:\n",
    "                pos_pmi = calculate_pmi(pos_tag_prob, pos_tag_pair_prob, tag1, tag2)\n",
    "                if pos_pmi > 0:\n",
    "                    pos_edges.append((phrase_tag_to_index[tag1], phrase_tag_to_index[tag2]))\n",
    "                    pos_edge_weights.append(pos_pmi)\n",
    "                    pos_edges.append((phrase_tag_to_index[tag2], phrase_tag_to_index[tag1]))\n",
    "                    pos_edge_weights.append(pos_pmi) # ppmi is symmetric\n",
    "    \n",
    "    if len(pos_edges) == 0:\n",
    "        pos_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        pos_edge_attr = torch.empty((0,), dtype=torch.float)\n",
    "    else:\n",
    "        pos_edge_index = torch.tensor(pos_edges, dtype=torch.long).t().contiguous()\n",
    "        pos_edge_attr = torch.tensor(pos_edge_weights, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    # Named Entity Recognition\n",
    "    \n",
    "    ner_edges = []\n",
    "    \n",
    "    if named_entities[idx]: # if there are entities\n",
    "        for i, entity in enumerate(named_entities[idx]):\n",
    "            ner_edges.append((named_entities_to_index[entity], named_entities_to_index[entity]))\n",
    "            \n",
    "    if len(ner_edges) == 0:\n",
    "        ner_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        ner_edge_index = torch.tensor(ner_edges, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "    # print(ppmi_edge_index.shape, ppmi_edge_attr.shape, dp_edge_index.shape, sem_edge_index.shape, sem_edge_attr.shape)\n",
    "    # x = torch.tensor([model.wv[word] for word in sentence], dtype=torch.float)\n",
    "    y = torch.tensor([labels[idx]])\n",
    "    \n",
    "    grouped_data = GroupedData(ppmi_x=word_embedding, \n",
    "                               ppmi_edge_index=ppmi_edge_index, \n",
    "                               ppmi_edge_attr=ppmi_edge_attr, \n",
    "                               dp_x=word_embedding, \n",
    "                               dp_edge_index=dp_edge_index,\n",
    "                               sem_x=phrase_embedding,\n",
    "                               sem_edge_index=sem_edge_index,\n",
    "                               sem_edge_attr=sem_edge_attr,\n",
    "                               pos_x=pos_emb,\n",
    "                               pos_edge_index=pos_edge_index,\n",
    "                               pos_edge_attr=pos_edge_attr,\n",
    "                               ner_x=ent_emb,\n",
    "                               ner_edge_index=ner_edge_index,\n",
    "                               y=y)\n",
    "    # print(grouped_data)\n",
    "    graphs.append(grouped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "505it [00:02, 194.51it/s]\n"
     ]
    }
   ],
   "source": [
    "test_graphs = []\n",
    "\n",
    "for idx, sentence in tqdm(enumerate(test_tokenized_corpus)):\n",
    "    \n",
    "    # PPMI\n",
    "    ppmi_edges = []\n",
    "    ppmi_edge_weights = []\n",
    "    \n",
    "    n = len(sentence)\n",
    "\n",
    "    for i, word1 in enumerate(sentence):\n",
    "        for j in range(i + 1, n):\n",
    "            word2 = sentence[j]\n",
    "            if word1 != word2:\n",
    "                pmi = calculate_pmi(test_word_prob, test_pair_prob, word1, word2)\n",
    "                if pmi > 0:\n",
    "                    ppmi_edges.append((test_word_to_index[word1], test_word_to_index[word2]))\n",
    "                    ppmi_edge_weights.append(pmi)\n",
    "                    ppmi_edges.append((test_word_to_index[word2], test_word_to_index[word1]))\n",
    "                    ppmi_edge_weights.append(pmi) # ppmi is symmetric\n",
    "    \n",
    "    if len(ppmi_edges) == 0:\n",
    "        ppmi_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        ppmi_edge_attr = torch.empty((0,), dtype=torch.float)\n",
    "    else:\n",
    "        ppmi_edge_index = torch.tensor(ppmi_edges, dtype=torch.long).t().contiguous()\n",
    "        ppmi_edge_attr = torch.tensor(ppmi_edge_weights, dtype=torch.float)\n",
    "\n",
    "    \n",
    "    # Dependency Parse\n",
    "    dp_edges = []\n",
    "    \n",
    "    doc = nlp(' '.join(sentence))\n",
    "    dependencies = [(token.text, token.head.text, token.dep_) for token in doc]\n",
    "    # dependencies = get_dependency_parse(text[idx])\n",
    "    for word, head, dep in dependencies:\n",
    "        if word != head and word.strip() and head.strip():  # Skip self-loops and empty tokens\n",
    "            dp_edges.append((test_word_to_index[head], test_word_to_index[word]))\n",
    "    \n",
    "    if len(dp_edges) == 0:\n",
    "        dp_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        dp_edge_index = torch.tensor(dp_edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    \n",
    "    # Phrase Embeddings\n",
    "    sem_edges = []\n",
    "    sem_edge_weights = []\n",
    "    \n",
    "    n_phrases = len(test_phrases[idx])\n",
    "\n",
    "    for i, phrase1 in enumerate(test_phrases[idx]):\n",
    "        for j in range(n_phrases):\n",
    "            phrase2 = test_phrases[idx][j]\n",
    "            if phrase1 != phrase2:\n",
    "                sem_edges.append((test_phrase_to_index[phrase1], test_phrase_to_index[phrase2]))\n",
    "                sem_edge_weights.append(test_cos_sim_matrix[test_phrase_to_index[phrase1], test_phrase_to_index[phrase2]])\n",
    "                sem_edges.append((test_phrase_to_index[phrase2], test_phrase_to_index[phrase1]))\n",
    "                sem_edge_weights.append(test_cos_sim_matrix[test_phrase_to_index[phrase2], test_phrase_to_index[phrase1]])\n",
    "    \n",
    "    if len(sem_edges) == 0:\n",
    "        sem_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        sem_edge_attr = torch.empty((0,), dtype=torch.float)\n",
    "    else:\n",
    "        sem_edge_index = torch.tensor(sem_edges, dtype=torch.long).t().contiguous()\n",
    "        sem_edge_attr = torch.tensor(sem_edge_weights, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    # Phrase tags\n",
    "    \n",
    "    pos_edges = []\n",
    "    pos_edge_weights = []\n",
    "    \n",
    "    n_phrase_tags = len(test_phrase_tags[idx])\n",
    "    \n",
    "    for i, tag1 in enumerate(test_phrase_tags[idx]):\n",
    "        for j in range(i + 1, n_phrase_tags):\n",
    "            tag2 = test_phrase_tags[idx][j]\n",
    "            if tag1 != tag2:\n",
    "                pos_pmi = calculate_pmi(test_pos_tag_prob, test_pos_tag_pair_prob, tag1, tag2)\n",
    "                if pos_pmi > 0:\n",
    "                    pos_edges.append((test_phrase_tag_to_index[tag1], test_phrase_tag_to_index[tag2]))\n",
    "                    pos_edge_weights.append(pos_pmi)\n",
    "                    pos_edges.append((test_phrase_tag_to_index[tag2], test_phrase_tag_to_index[tag1]))\n",
    "                    pos_edge_weights.append(pos_pmi) # ppmi is symmetric\n",
    "    \n",
    "    if len(pos_edges) == 0:\n",
    "        pos_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        pos_edge_attr = torch.empty((0,), dtype=torch.float)\n",
    "    else:\n",
    "        pos_edge_index = torch.tensor(pos_edges, dtype=torch.long).t().contiguous()\n",
    "        pos_edge_attr = torch.tensor(pos_edge_weights, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    # Named Entity Recognition\n",
    "    \n",
    "    ner_edges = []\n",
    "    \n",
    "    if test_named_entities[idx]: # if there are entities\n",
    "        for i, entity in enumerate(test_named_entities[idx]):\n",
    "            ner_edges.append((test_named_entities_to_index[entity], test_named_entities_to_index[entity]))\n",
    "            \n",
    "    if len(ner_edges) == 0: # empty graph if there are no entities\n",
    "        ner_edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        ner_edge_index = torch.tensor(ner_edges, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "    # print(ppmi_edge_index.shape, ppmi_edge_attr.shape, dp_edge_index.shape, sem_edge_index.shape, sem_edge_attr.shape)\n",
    "    # x = torch.tensor([model.wv[word] for word in sentence], dtype=torch.float)\n",
    "    y = torch.tensor([test_labels[idx]])\n",
    "    \n",
    "    grouped_data = GroupedData(ppmi_x=word_embedding, \n",
    "                               ppmi_edge_index=ppmi_edge_index, \n",
    "                               ppmi_edge_attr=ppmi_edge_attr, \n",
    "                               dp_x=word_embedding, \n",
    "                               dp_edge_index=dp_edge_index,\n",
    "                               sem_x=phrase_embedding,\n",
    "                               sem_edge_index=sem_edge_index,\n",
    "                               sem_edge_attr=sem_edge_attr,\n",
    "                               pos_x=pos_emb,\n",
    "                               pos_edge_index=pos_edge_index,\n",
    "                               pos_edge_attr=pos_edge_attr,\n",
    "                               ner_x=ent_emb,\n",
    "                               ner_edge_index=ner_edge_index,\n",
    "                               y=y)\n",
    "    # print(grouped_data)\n",
    "    test_graphs.append(grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verb 0\n",
    "adj 1\n",
    "noun 2\n",
    "\n",
    "'adj_noun' [0000010000] -> [0110000...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv, SAGPooling, global_max_pool\n",
    "# from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "# # Define a single GCN \n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels, out_pooling):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "#         self.sag_pooling = SAGPooling(out_channels, ratio=out_pooling, GNN=GCNConv)\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_weight=None, batch=None):\n",
    "#         x = self.conv1(x, edge_index, edge_weight)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index, edge_weight)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv3(x, edge_index, edge_weight)\n",
    "#         # x = self.sag_pooling(x, edge_index, edge_weight, batch)\n",
    "#         x = self.maxpooling(x, batch)\n",
    "#         # print(x[0].shape)\n",
    "#         return x\n",
    "\n",
    "# # Final model combining multiple GCNs and a classification layer\n",
    "# class MultiGCNClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_emb_sizes, hidden_channels, out_channels, out_pooling, num_gcns, num_classes):\n",
    "#         super(MultiGCNClassifier, self).__init__()\n",
    "#         self.out_pooling = out_pooling\n",
    "#         self.num_gcns = num_gcns\n",
    "#         self.out_channels = out_channels\n",
    "        \n",
    "#         self.ppmi_gcn = GCN(input_emb_sizes['ppmi'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.dp_gcn = GCN(input_emb_sizes['dp'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.sem_gcn = GCN(input_emb_sizes['sem'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.pos_gcn = GCN(input_emb_sizes['pos'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.ner_gcn = GCN(input_emb_sizes['ner'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.linear = torch.nn.Linear(out_pooling*out_channels*num_gcns, num_classes)\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         embeddings = []\n",
    "#         embeddings.append(self.ppmi_gcn(batch.ppmi_x, batch.ppmi_edge_index, batch.ppmi_edge_attr, batch=batch.ppmi_x_batch))\n",
    "#         embeddings.append(self.dp_gcn(batch.dp_x, batch.dp_edge_index, batch=batch.dp_x_batch))\n",
    "#         embeddings.append(self.sem_gcn(batch.sem_x, batch.sem_edge_index, batch.sem_edge_attr, batch=batch.sem_x_batch))\n",
    "#         embeddings.append(self.pos_gcn(batch.pos_x, batch.pos_edge_index, batch.pos_edge_attr, batch=batch.pos_x_batch))\n",
    "#         embeddings.append(self.ner_gcn(batch.ner_x, batch.ner_edge_index, batch=batch.ner_x_batch))\n",
    "        \n",
    "#         # Concatenate the embeddings from each GCN\n",
    "#         concatenated = torch.cat(embeddings, dim=1)\n",
    "#         # reshape to (batch_size, out_pooling, num_gcns*out_channels)\n",
    "#         concatenated = concatenated.reshape(batch.y.shape[0], self.out_pooling*self.num_gcns*self.out_channels)\n",
    "#         # apply linear layer\n",
    "#         out = self.linear(concatenated)\n",
    "#         return F.log_softmax(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combined graph model\n",
    "# class CombinedGraphGCN(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, hidden_channels, out_channels, out_pooling, num_gcns, num_classes):\n",
    "#         super(CombinedGraphGCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(input_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "#         self.pooling = SAGPooling(out_channels, ratio=out_pooling, GNN=GCNConv)\n",
    "#         self.linear = torch.nn.Linear(out_pooling*out_channels, num_classes)\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         x = self.conv1(batch.x, batch.edge_index, batch.edge_attr)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, batch.edge_index, batch.edge_attr)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv3(x, batch.edge_index, batch.edge_attr)\n",
    "#         x = self.pooling(x, batch.edge_index, batch.edge_attr)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### num_classes\n",
    "\n",
    "TREC coarse : 6\n",
    "\n",
    "ARC : 3\n",
    "\n",
    "ARG: 4\n",
    "\n",
    "NU : 3\n",
    "\n",
    "LREC : 3\n",
    "\n",
    "BLOOM : 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try different pooling\n",
    "\n",
    "try another small dataset \n",
    "- tried NU\n",
    "\n",
    "try MLP instead \n",
    "\n",
    "try batch size 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, validation_set = train_test_split(graphs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, optimizer, and loss function\n",
    "num_gcns = 5\n",
    "hidden_channels = 64\n",
    "out_channels = 32\n",
    "out_pooling = 32\n",
    "num_classes = 6 # change according to dataset\n",
    "batch_size = 2\n",
    "dropout = 0.1\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, follow_batch=['ppmi_x', 'dp_x', 'sem_x', 'pos_x', 'ner_x'])\n",
    "val_loader = DataLoader(validation_set, batch_size=len(validation_set), follow_batch=['ppmi_x', 'dp_x', 'sem_x', 'pos_x', 'ner_x'])\n",
    "test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), follow_batch=['ppmi_x', 'dp_x', 'sem_x', 'pos_x', 'ner_x'])\n",
    "\n",
    "# train_loader = DataLoader(train_set, batch_size=batch_size, )\n",
    "# val_loader = DataLoader(validation_set, batch_size=len(validation_set), )\n",
    "# test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), )\n",
    "\n",
    "\n",
    "# obtain embedding sizes from the first batch\n",
    "input_emb_sizes = {}\n",
    "batch = next(iter(train_loader))\n",
    "input_emb_sizes['ppmi'] = batch.ppmi_x.shape[1]\n",
    "input_emb_sizes['dp'] = batch.dp_x.shape[1]\n",
    "input_emb_sizes['sem'] = batch.sem_x.shape[1]\n",
    "input_emb_sizes['pos'] = batch.pos_x.shape[1]\n",
    "input_emb_sizes['ner'] = batch.ner_x.shape[1]\n",
    "\n",
    "# model = MultiGCNClassifier(input_emb_sizes=input_emb_sizes, \n",
    "#                            hidden_channels=hidden_channels, \n",
    "#                            out_channels=out_channels, \n",
    "#                            num_gcns=num_gcns,\n",
    "#                            out_pooling=out_pooling, \n",
    "#                            num_classes=num_classes)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedDataBatch(y=[2], ppmi_x=[9018, 100], ppmi_x_batch=[9018], ppmi_x_ptr=[3], ppmi_edge_index=[2, 206], ppmi_edge_attr=[206], dp_x=[9018, 100], dp_x_batch=[9018], dp_x_ptr=[3], dp_edge_index=[2, 0], sem_x=[17450, 768], sem_x_batch=[17450], sem_x_ptr=[3], sem_edge_index=[2, 80], sem_edge_attr=[80], pos_x=[712, 356], pos_x_batch=[712], pos_x_ptr=[3], pos_edge_index=[2, 0], pos_edge_attr=[0], ner_x=[1140, 100], ner_x_batch=[1140], ner_x_ptr=[3], ner_edge_index=[2, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv, SAGPooling, global_max_pool\n",
    "# from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "# # Define a single GCN \n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels, out_pooling):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "#         self.sag_pooling = SAGPooling(out_channels, ratio=out_pooling)\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_weight=None, batch=None):\n",
    "#         x = self.conv1(x, edge_index, edge_weight)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index, edge_weight)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv3(x, edge_index, edge_weight)\n",
    "#         x = F.relu(x)\n",
    "#         x, edge_index, edge_weight, batch, _, _ = self.sag_pooling(x, edge_index, edge_weight, batch)\n",
    "#         x = global_max_pool(x, batch)\n",
    "#         return x\n",
    "\n",
    "# # Final model combining multiple GCNs and a classification layer\n",
    "# class MultiGCNClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_emb_sizes, hidden_channels, out_channels, out_pooling, num_gcns, num_classes):\n",
    "#         super(MultiGCNClassifier, self).__init__()\n",
    "#         self.out_pooling = out_pooling\n",
    "#         self.num_gcns = num_gcns\n",
    "#         self.out_channels = out_channels\n",
    "        \n",
    "#         self.ppmi_gcn = GCN(input_emb_sizes['ppmi'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.dp_gcn = GCN(input_emb_sizes['dp'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.sem_gcn = GCN(input_emb_sizes['sem'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.pos_gcn = GCN(input_emb_sizes['pos'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.ner_gcn = GCN(input_emb_sizes['ner'], hidden_channels, out_channels, out_pooling)\n",
    "#         self.linear = torch.nn.Linear(num_gcns*out_channels, num_classes)\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         embeddings = []\n",
    "#         embeddings.append(self.ppmi_gcn(batch.ppmi_x, batch.ppmi_edge_index, batch.ppmi_edge_attr, batch=batch.ppmi_x_batch))\n",
    "#         embeddings.append(self.dp_gcn(batch.dp_x, batch.dp_edge_index, batch=batch.dp_x_batch))\n",
    "#         embeddings.append(self.sem_gcn(batch.sem_x, batch.sem_edge_index, batch.sem_edge_attr, batch=batch.sem_x_batch))\n",
    "#         embeddings.append(self.pos_gcn(batch.pos_x, batch.pos_edge_index, batch.pos_edge_attr, batch=batch.pos_x_batch))\n",
    "#         embeddings.append(self.ner_gcn(batch.ner_x, batch.ner_edge_index, batch=batch.ner_x_batch))\n",
    "        \n",
    "#         # Concatenate the embeddings from each GCN\n",
    "#         concatenated = torch.cat(embeddings, dim=1)\n",
    "#         concatenated = concatenated.reshape(batch.y.shape[0], self.num_gcns*self.out_channels)\n",
    "        \n",
    "#         # Apply linear layer\n",
    "#         out = self.linear(concatenated)\n",
    "#         return F.log_softmax(out, dim=1)\n",
    "\n",
    "# # Example training loop\n",
    "# def train(model, data_loader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     for batch in data_loader:\n",
    "#         batch = batch.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(batch)\n",
    "#         loss = criterion(out, batch.y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print('Train Loss:', loss.item())\n",
    "\n",
    "# # Example data loader and training call\n",
    "# # Ensure your data is prepared and loaded correctly\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = MultiGCNClassifier(input_emb_sizes, hidden_channels, out_channels, out_pooling, num_gcns, num_classes).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# num_epochs = 10\n",
    "# train_loss = []\n",
    "\n",
    "# # # Assuming `train_loader` is your DataLoader\n",
    "# # for epoch in range(num_epochs):\n",
    "# #     train(model, train_loader, optimizer, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 1/100, Train Loss: 1.6655, Validation Loss: 1.6123, Validation Accuracy: 0.3911\n",
      "Epoch 2/100, Train Loss: 1.5352, Validation Loss: 1.4895, Validation Accuracy: 0.4406\n",
      "Epoch 3/100, Train Loss: 1.4110, Validation Loss: 1.4440, Validation Accuracy: 0.4703\n",
      "Epoch 4/100, Train Loss: 1.2985, Validation Loss: 1.3867, Validation Accuracy: 0.4802\n",
      "Epoch 5/100, Train Loss: 1.1886, Validation Loss: 1.3533, Validation Accuracy: 0.4926\n",
      "Epoch 6/100, Train Loss: 1.1043, Validation Loss: 1.3379, Validation Accuracy: 0.4926\n",
      "Epoch 7/100, Train Loss: 1.0385, Validation Loss: 1.3180, Validation Accuracy: 0.5124\n",
      "Epoch 8/100, Train Loss: 0.9740, Validation Loss: 1.3211, Validation Accuracy: 0.5000\n",
      "Epoch 9/100, Train Loss: 0.9144, Validation Loss: 1.3374, Validation Accuracy: 0.5124\n",
      "Epoch 10/100, Train Loss: 0.8591, Validation Loss: 1.3684, Validation Accuracy: 0.5124\n",
      "Epoch 11/100, Train Loss: 0.8128, Validation Loss: 1.3882, Validation Accuracy: 0.5025\n",
      "Epoch 12/100, Train Loss: 0.7551, Validation Loss: 1.4223, Validation Accuracy: 0.5050\n",
      "Epoch 13/100, Train Loss: 0.7226, Validation Loss: 1.4199, Validation Accuracy: 0.5223\n",
      "Epoch 14/100, Train Loss: 0.6906, Validation Loss: 1.4001, Validation Accuracy: 0.5619\n",
      "Epoch 15/100, Train Loss: 0.6432, Validation Loss: 1.4076, Validation Accuracy: 0.5718\n",
      "Epoch 16/100, Train Loss: 0.6211, Validation Loss: 1.4199, Validation Accuracy: 0.5693\n",
      "Epoch 17/100, Train Loss: 0.6027, Validation Loss: 1.4336, Validation Accuracy: 0.5693\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, SAGPooling, global_max_pool, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from NodeNorm.layers import NodeNorm\n",
    "import numpy as np\n",
    "\n",
    "# Define a single GCN \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, out_pooling, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.nodenorm = NodeNorm(nn_type='n')\n",
    "        self.sag_pooling = SAGPooling(out_channels, ratio=out_pooling) # separate attention pooling for words and phrases\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None, batch=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        # x = self.nodenorm(x)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # x = self.conv2(x, edge_index, edge_weight)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        # x = self.nodenorm(x)\n",
    "        x = F.relu(x)\n",
    "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x, edge_index, edge_weight, batch, _, _ = self.sag_pooling(x, edge_index, edge_weight, batch)\n",
    "        x = global_max_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "# Final model combining multiple GCNs and a classification layer\n",
    "class MultiGCNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_emb_sizes, hidden_channels, out_channels, out_pooling, num_gcns, num_classes, dropout):\n",
    "        super(MultiGCNClassifier, self).__init__()\n",
    "        self.out_pooling = out_pooling\n",
    "        self.num_gcns = num_gcns\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.ppmi_gcn = GCN(input_emb_sizes['ppmi'], hidden_channels, out_channels, out_pooling, dropout)\n",
    "        self.dp_gcn = GCN(input_emb_sizes['dp'], hidden_channels, out_channels, out_pooling, dropout)\n",
    "        self.sem_gcn = GCN(input_emb_sizes['sem'], hidden_channels, out_channels, out_pooling, dropout)\n",
    "        self.pos_gcn = GCN(input_emb_sizes['pos'], hidden_channels, out_channels, out_pooling, dropout)\n",
    "        self.ner_gcn = GCN(input_emb_sizes['ner'], hidden_channels, out_channels, out_pooling, dropout)\n",
    "\n",
    "        self.conv2d = nn.Conv1d(num_gcns, 1, kernel_size=3, stride=1)\n",
    "        self.linear = torch.nn.Linear((out_channels*num_gcns), num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        embeddings = []\n",
    "        embeddings.append(self.ppmi_gcn(batch.ppmi_x, batch.ppmi_edge_index, batch.ppmi_edge_attr, batch=batch.ppmi_x_batch))\n",
    "        embeddings.append(self.dp_gcn(batch.dp_x, batch.dp_edge_index, batch=batch.dp_x_batch))\n",
    "        embeddings.append(self.sem_gcn(batch.sem_x, batch.sem_edge_index, batch.sem_edge_attr, batch=batch.sem_x_batch))\n",
    "        embeddings.append(self.pos_gcn(batch.pos_x, batch.pos_edge_index, batch.pos_edge_attr, batch=batch.pos_x_batch))\n",
    "        embeddings.append(self.ner_gcn(batch.ner_x, batch.ner_edge_index, batch=batch.ner_x_batch))\n",
    "        \n",
    "        # Concatenate the embeddings from each GCN\n",
    "        concatenated = torch.cat(embeddings, dim=1)\n",
    "        concatenated = concatenated.reshape(batch.y.shape[0], self.num_gcns*self.out_channels)\n",
    "        # concatenated = concatenated.reshape(batch.y.shape[0], self.num_gcns, self.out_channels)\n",
    "\n",
    "        # # Apply 2D Convolutional layer\n",
    "        # conv_output = self.conv2d(concatenated)\n",
    "        # conv_output = conv_output.reshape(batch.y.shape[0], -1)\n",
    "        # # print(conv_output.shape)\n",
    "        \n",
    "        # Apply linear layer\n",
    "        out = self.linear(concatenated) # convolution instead\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "# Training loop with validation, learning rate warm-up, and early stopping\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, scheduler, device, num_epochs, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # # Learning rate warm-up\n",
    "        # scheduler.step()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch)\n",
    "                loss = criterion(out, batch.y)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += pred.eq(batch.y).sum().item()\n",
    "                total += batch.y.size(0)\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {np.mean(train_losses):.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping!')\n",
    "                model.load_state_dict(torch.load('best_model.pth'))\n",
    "                break\n",
    "\n",
    "# Example data loader and training call\n",
    "# Ensure your data is prepared and loaded correctly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = MultiGCNClassifier(input_emb_sizes, hidden_channels, out_channels, out_pooling, num_gcns, num_classes, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, criterion, scheduler, device, num_epochs, patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score (Micro): 0.2634\n",
      "Test F1 Score (Macro): 0.1551\n",
      "Test Precision (Micro): 0.2634\n",
      "Test Precision (Macro): 0.1693\n",
      "Test Recall (Micro): 0.2634\n",
      "Test Recall (Macro): 0.1702\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(batch.y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Test F1 Score (Micro): {f1_micro:.4f}')\n",
    "    print(f'Test F1 Score (Macro): {f1_macro:.4f}')\n",
    "    print(f'Test Precision (Micro): {precision_micro:.4f}')\n",
    "    print(f'Test Precision (Macro): {precision_macro:.4f}')\n",
    "    print(f'Test Recall (Micro): {recall_micro:.4f}')\n",
    "    print(f'Test Recall (Macro): {recall_macro:.4f}')\n",
    "\n",
    "    return {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"recall_macro\": recall_macro\n",
    "    }\n",
    "\n",
    "# Example test call\n",
    "# Assuming `test_loader` is your DataLoader for the test set\n",
    "test_metrics = test(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_recall, multiclass_precision, multiclass_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:04<00:00, 19.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.8622546195983887\n",
      "val_acc: 0.5000, val_loss: 0.9931, val_micro_f1: 0.5000, val_macro_f1: 0.2222, val_micro_precision: 0.5000, val_macro_precision: 0.1667, val_micro_recall: 0.5000, val_macro_recall: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:05<00:00, 15.86it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.046687126159668\n",
      "val_acc: 0.6190, val_loss: 0.8585, val_micro_f1: 0.6190, val_macro_f1: 0.4607, val_micro_precision: 0.6190, val_macro_precision: 0.4114, val_micro_recall: 0.6190, val_macro_recall: 0.5368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:05<00:00, 15.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.4766135811805725\n",
      "val_acc: 0.6667, val_loss: 0.7929, val_micro_f1: 0.6667, val_macro_f1: 0.4938, val_micro_precision: 0.6667, val_macro_precision: 0.4431, val_micro_recall: 0.6667, val_macro_recall: 0.5578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:05<00:00, 15.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.37650415301322937\n",
      "val_acc: 0.6667, val_loss: 0.7720, val_micro_f1: 0.6667, val_macro_f1: 0.5248, val_micro_precision: 0.6667, val_macro_precision: 0.7721, val_micro_recall: 0.6667, val_macro_recall: 0.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:05<00:00, 16.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.2880127727985382\n",
      "val_acc: 0.6429, val_loss: 0.7853, val_micro_f1: 0.6429, val_macro_f1: 0.5568, val_micro_precision: 0.6429, val_macro_precision: 0.5998, val_micro_recall: 0.6429, val_macro_recall: 0.5769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:06<00:00, 13.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.15373949706554413\n",
      "val_acc: 0.6190, val_loss: 0.7987, val_micro_f1: 0.6190, val_macro_f1: 0.5795, val_micro_precision: 0.6190, val_macro_precision: 0.5741, val_micro_recall: 0.6190, val_macro_recall: 0.5907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:05<00:00, 16.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.09430237859487534\n",
      "val_acc: 0.6190, val_loss: 0.8760, val_micro_f1: 0.6190, val_macro_f1: 0.6070, val_micro_precision: 0.6190, val_macro_precision: 0.6123, val_micro_recall: 0.6190, val_macro_recall: 0.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:05<00:00, 14.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.03761705756187439\n",
      "val_acc: 0.6429, val_loss: 0.8959, val_micro_f1: 0.6429, val_macro_f1: 0.6336, val_micro_precision: 0.6429, val_macro_precision: 0.6412, val_micro_recall: 0.6429, val_macro_recall: 0.6595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:07<00:00, 11.70it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.014343492686748505\n",
      "val_acc: 0.5714, val_loss: 0.9929, val_micro_f1: 0.5714, val_macro_f1: 0.5712, val_micro_precision: 0.5714, val_macro_precision: 0.6004, val_micro_recall: 0.5714, val_macro_recall: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:06<00:00, 13.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.012009811587631702\n",
      "val_acc: 0.5595, val_loss: 1.0497, val_micro_f1: 0.5595, val_macro_f1: 0.5476, val_micro_precision: 0.5595, val_macro_precision: 0.5624, val_micro_recall: 0.5595, val_macro_recall: 0.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for train_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process each batch through the respective GCN\n",
    "        out = model(train_batch)\n",
    "        \n",
    "        # Use the label to calculate loss calculation\n",
    "        train_loss = criterion(out, train_batch.y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for val_batch in tqdm(val_loader):\n",
    "        out = model(val_batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == val_batch.y).sum().item()\n",
    "        total += val_batch.y.size(0)\n",
    "    \n",
    "    val_loss = F.nll_loss(out, val_batch.y)\n",
    "    val_acc = correct / total\n",
    "    val_micro_f1 = multiclass_f1_score(out, val_batch.y, average='micro', num_classes=num_classes)\n",
    "    val_macro_f1 = multiclass_f1_score(out, val_batch.y, average='macro', num_classes=num_classes)\n",
    "    val_macro_precision = multiclass_precision(out, val_batch.y, average='macro', num_classes=num_classes)\n",
    "    val_micro_precision = multiclass_precision(out, val_batch.y, average='micro', num_classes=num_classes)\n",
    "    val_micro_recall = multiclass_recall(out, val_batch.y, average='micro', num_classes=num_classes)\n",
    "    val_macro_recall = multiclass_recall(out, val_batch.y, average='macro', num_classes=num_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss.item()}')\n",
    "    print(f'val_acc: {val_acc:.4f}, val_loss: {val_loss:.4f}, val_micro_f1: {val_micro_f1:.4f}, val_macro_f1: {val_macro_f1:.4f}, val_micro_precision: {val_micro_precision:.4f}, val_macro_precision: {val_macro_precision:.4f}, val_micro_recall: {val_micro_recall:.4f}, val_macro_recall: {val_macro_recall:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.3184, test_micro_f1: 0.3184, test_macro_f1: 0.2980, test_micro_precision: 0.3184, test_macro_precision: 0.3422, test_micro_recall: 0.3184, test_macro_recall: 0.3108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for test_batch in tqdm(test_loader):\n",
    "    out = model(test_batch)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct += (pred == test_batch.y).sum().item()\n",
    "    total += test_batch.y.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "\n",
    "test_micro_f1 = multiclass_f1_score(out, test_batch.y, average='micro', num_classes=num_classes)\n",
    "test_macro_f1 = multiclass_f1_score(out, test_batch.y, average='macro', num_classes=num_classes)\n",
    "\n",
    "test_micro_precision = multiclass_precision(out, test_batch.y, average='micro', num_classes=num_classes)\n",
    "test_macro_precision = multiclass_precision(out, test_batch.y, average='macro', num_classes=num_classes)\n",
    "\n",
    "test_micro_recall = multiclass_recall(out, test_batch.y, average='micro', num_classes=num_classes)\n",
    "test_macro_recall = multiclass_recall(out, test_batch.y, average='macro', num_classes=num_classes)\n",
    "print(f'test_acc: {test_acc:.4f}, test_micro_f1: {test_micro_f1:.4f}, test_macro_f1: {test_macro_f1:.4f}, test_micro_precision: {test_micro_precision:.4f}, test_macro_precision: {test_macro_precision:.4f}, test_micro_recall: {test_micro_recall:.4f}, test_macro_recall: {test_macro_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".shine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
