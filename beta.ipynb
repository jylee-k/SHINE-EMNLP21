{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocess/trec_split.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_data = data['train']\n",
    "test_data = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'How did serfdom develop in and then leave Russia ?',\n",
       " 'coarse_label': 2,\n",
       " 'fine_label': 26}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to form any kinds of graph, you first need to go through the entire corpus and obtain {idx:node}. we first do this for word, then pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaning text\n",
    "import string\n",
    "def clean_str(sentence ,use=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    if not use: return sentence\n",
    "\n",
    "    sentence = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" \\'s\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" \\'ve\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" n\\'t\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" \\'re\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" \\'d\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" \\'ll\", sentence)\n",
    "    sentence = re.sub(r\",\", \" , \", sentence)\n",
    "    sentence = re.sub(r\"!\", \" ! \", sentence)\n",
    "    sentence = re.sub(r\"\\(\", \" \\( \", sentence)\n",
    "    sentence = re.sub(r\"\\)\", \" \\) \", sentence)\n",
    "    sentence = re.sub(r\"\\?\", \" \\? \", sentence)\n",
    "    sentence = re.sub(r\"\\s{2,}\", \" \", sentence)\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return sentence.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI Adjacency Matrix:\n",
      "[[0.         2.83321334 2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.83321334 0.         2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.14006616 2.14006616 0.         2.14006616 2.14006616 2.14006616]\n",
      " [1.73460106 1.73460106 2.14006616 0.         1.73460106 1.73460106]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 0.         2.83321334]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 2.83321334 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    unique_words = set()\n",
    "    word_count = Counter()\n",
    "    pair_count = defaultdict(int)\n",
    "    total_words = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        line = clean_str(line)\n",
    "        words = line.split()\n",
    "        total_words += len(words)\n",
    "        word_count.update(words)\n",
    "        for i, word in enumerate(words):\n",
    "            unique_words.add(word)\n",
    "            for j in range(i + 1, len(words)):\n",
    "                pair = tuple(sorted([word, words[j]]))\n",
    "                pair_count[pair] += 1\n",
    "    \n",
    "    word_prob = {word: count / total_words for word, count in word_count.items()}\n",
    "    pair_prob = {pair: count / total_words for pair, count in pair_count.items()}\n",
    "    \n",
    "    return word_prob, pair_prob, unique_words\n",
    "\n",
    "def calculate_pmi(word_prob, pair_prob, word1, word2):\n",
    "    pair = tuple(sorted([word1, word2]))\n",
    "    if pair in pair_prob and word1 in word_prob and word2 in word_prob:\n",
    "        pmi = log(pair_prob[pair] / (word_prob[word1] * word_prob[word2]))\n",
    "        return pmi\n",
    "    return 0.0\n",
    "\n",
    "def create_pmi_matrix(sentence, word_prob, pair_prob, word_index):\n",
    "    words = clean_str(sentence).split()\n",
    "    n = len(words)\n",
    "    pmi_matrix = np.zeros((n, n))\n",
    "    node_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            node_list.append(word_index[word])\n",
    "        else:\n",
    "            node_list.append(-1)\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pmi = calculate_pmi(word_prob, pair_prob, words[i], words[j])\n",
    "            pmi_matrix[i, j] = pmi\n",
    "            pmi_matrix[j, i] = pmi  # PMI matrix is symmetric\n",
    "    \n",
    "    return pmi_matrix, node_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"Another line; with more: punctuation.\",\n",
    "    \"Is this working? Yes, it is!\"\n",
    "]\n",
    "\n",
    "word_prob, pair_prob, unique_words = process_corpus(corpus)\n",
    "word_index = {word: index for index, word in enumerate(sorted(unique_words))}\n",
    "\n",
    "sentence = \"Hello world, this is a test\"\n",
    "pmi_matrix, node_list = create_pmi_matrix(sentence, word_prob, pair_prob, word_index)\n",
    "print(\"PMI Adjacency Matrix:\")\n",
    "print(pmi_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'another': 1,\n",
       " 'hello': 2,\n",
       " 'is': 3,\n",
       " 'it': 4,\n",
       " 'line': 5,\n",
       " 'more': 6,\n",
       " 'punctuation': 7,\n",
       " 'test': 8,\n",
       " 'this': 9,\n",
       " 'with': 10,\n",
       " 'working': 11,\n",
       " 'world': 12,\n",
       " 'yes': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI Adjacency Matrix:\n",
      "[[0.         2.83321334 2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.83321334 0.         2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.14006616 2.14006616 0.         2.14006616 2.14006616 2.14006616]\n",
      " [1.73460106 1.73460106 2.14006616 0.         1.73460106 1.73460106]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 0.         2.83321334]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 2.83321334 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def process_corpus_tags(corpus):\n",
    "    unique_tags = set()\n",
    "    tag_count = Counter()\n",
    "    tag_pair_count = defaultdict(int)\n",
    "    total_tags = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        line = clean_str(line)\n",
    "        # get pos tags for words in the query\n",
    "        tags = [one[1].lower() for one in nltk.pos_tag(nltk.word_tokenize(line))]\n",
    "        if '' in tags:\n",
    "            print(line)\n",
    "        tags = line.split()\n",
    "        total_tags += len(tags)\n",
    "        tag_count.update(tags)\n",
    "        for i, tag in enumerate(tags):\n",
    "            unique_tags.add(tag)\n",
    "            for j in range(i + 1, len(tags)):\n",
    "                pair = tuple(sorted([tag, tags[j]]))\n",
    "                tag_pair_count[pair] += 1\n",
    "    \n",
    "    tag_prob = {tag: count / total_tags for tag, count in tag_count.items()}\n",
    "    pair_prob = {pair: count / total_tags for pair, count in tag_pair_count.items()}\n",
    "    \n",
    "    return tag_prob, tag_pair_prob, unique_tags\n",
    "\n",
    "def calculate_pmi(word_prob, pair_prob, word1, word2):\n",
    "    pair = tuple(sorted([word1, word2]))\n",
    "    if pair in pair_prob and word1 in word_prob and word2 in word_prob:\n",
    "        pmi = log(pair_prob[pair] / (word_prob[word1] * word_prob[word2]))\n",
    "        return pmi\n",
    "    return 0.0\n",
    "\n",
    "def create_pmi_matrix(sentence, tag_prob, tag_pair_prob, tag_index):\n",
    "    words = clean_str(sentence).split()\n",
    "    n = len(words)\n",
    "    pmi_matrix = np.zeros((n, n))\n",
    "    node_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            node_list.append(word_index[word])\n",
    "        else:\n",
    "            node_list.append(-1)\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pmi = calculate_pmi(word_prob, pair_prob, words[i], words[j])\n",
    "            pmi_matrix[i, j] = pmi\n",
    "            pmi_matrix[j, i] = pmi  # PMI matrix is symmetric\n",
    "    \n",
    "    return pmi_matrix, node_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"Another line; with more: punctuation.\",\n",
    "    \"Is this working? Yes, it is!\"\n",
    "]\n",
    "\n",
    "tag_prob, tag_pair_prob, unique_tags = process_corpus_tags(corpus)\n",
    "tag_index = {tag: index for index, tag in enumerate(sorted(unique_tags))}\n",
    "\n",
    "sentence = \"Hello world, this is a test\"\n",
    "pmi_matrix, node_list = create_pmi_matrix(sentence, word_prob, pair_prob, word_index)\n",
    "print(\"PMI Adjacency Matrix:\")\n",
    "print(pmi_matrix)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### euclidean distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between the vectors is: 5.196152422706632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two embedding vectors.\n",
    "    \n",
    "    Args:\n",
    "    vector1 (np.array): First embedding vector.\n",
    "    vector2 (np.array): Second embedding vector.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Euclidean distance between the two vectors.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "# Example usage\n",
    "embedding1 = np.array([1, 2, 3])\n",
    "embedding2 = np.array([4, 5, 6])\n",
    "\n",
    "distance = euclidean_distance(embedding1, embedding2)\n",
    "print(f\"The Euclidean distance between the vectors is: {distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Ensure you have the necessary nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'NOUN'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
      "Phrases: ['dog', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words, tagset='universal')\n",
    "    return pos_tags\n",
    "\n",
    "def identify_phrases(pos_tags, np_pattern, vp_pattern):\n",
    "    pos_sequence = ' '.join([tag for word, tag in pos_tags])\n",
    "    \n",
    "    np_regex = re.compile(np_pattern)\n",
    "    vp_regex = re.compile(vp_pattern)\n",
    "    \n",
    "    np_matches = [(match.start(), match.end()) for match in np_regex.finditer(pos_sequence)]\n",
    "    vp_matches = [(match.start(), match.end()) for match in vp_regex.finditer(pos_sequence)]\n",
    "    \n",
    "    phrases = []\n",
    "    for start, end in sorted(np_matches + vp_matches):\n",
    "        phrase = ' '.join([word for word, tag in pos_tags[start:end]])\n",
    "        phrases.append(phrase)\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "# Define the patterns\n",
    "np_pattern = r\"((DET)?(NUM)*((ADJ)(PUNCT)?(CONJ)?)*(((NOUN)|(PROPN))(PART)?)+)\"\n",
    "vp_pattern = r\"((AUX)*(ADV)*(VERB))\"\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "pos_tags = pos_tag_sentence(sentence)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "phrases = identify_phrases(pos_tags, np_pattern, vp_pattern)\n",
    "print(\"Phrases:\", phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP with spacy model (using this for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox\n",
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "sample_text = 'The quick brown fox jumps over the lazy dog'\n",
    "sample_doc = nlp(sample_text)\n",
    "# Extract Noun Phrases\n",
    "for chunk in sample_doc.noun_chunks:\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP with spacy model (obtained from fyp student) (using this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox jumps\n",
      "jumps over\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "sample_text = ('The quick brown fox jumps over the lazy dog')\n",
    "expression = r'(<VERB>?<ADV>*<VERB>+)'\n",
    "# pattern = [{\"TEXT\": {\"REGEX\": '(<VERB>?<ADV>*<VERB>+)'}}]\n",
    "vp_patterns = [\n",
    "    [{\"POS\": \"ADV\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"PRON\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}, {\"POS\": \"ADP\"}]\n",
    "]\n",
    "\n",
    "\n",
    "# get_verb_phrases = textacy.extract.token_matches(sample_text, patterns=patterns)\n",
    "# verb_phrases = []\n",
    "# for verb_phrase in get_verb_phrases:\n",
    "#     verb_phrases.append(verb_phrase)\n",
    "sample_doc = textacy.make_spacy_doc(sample_text,\n",
    "                                        lang='en_core_web_lg')\n",
    "verb_phrases = textacy.extract.token_matches(sample_doc, vp_patterns)\n",
    "# Print all Verb Phrase\n",
    "for chunk in verb_phrases:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in re.finditer(expression, sample_doc.text):\n",
    "    start, end = match.span()\n",
    "    span = sample_doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP with spacy using grammar tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb Phrases: ['He is eating an apple while she reads a book .', 'while she reads a book']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_verb_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    verb_phrases = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb_phrase = ' '.join([child.text for child in token.subtree])\n",
    "            verb_phrases.append(verb_phrase)\n",
    "    return verb_phrases\n",
    "\n",
    "sentence = \"He is eating an apple while she reads a book.\"\n",
    "verb_phrases = extract_verb_phrases(sentence)\n",
    "print(\"Verb Phrases:\", verb_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VP using nltk regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jylee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb Phrases: ['is eating an apple while she reads a book .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_verb_phrases(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    chunk_grammar = \"VP: {<VB.*><.*>*}\"\n",
    "    chunk_parser = RegexpParser(chunk_grammar)\n",
    "    tree = chunk_parser.parse(tagged)\n",
    "\n",
    "    verb_phrases = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == \"VP\":\n",
    "            verb_phrase = ' '.join(word for word, pos in subtree.leaves())\n",
    "            verb_phrases.append(verb_phrase)\n",
    "    return verb_phrases\n",
    "\n",
    "sentence = \"He is eating an apple while she reads a book.\"\n",
    "verb_phrases = extract_verb_phrases(sentence)\n",
    "print(\"Verb Phrases:\", verb_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined phrase (NP and VP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_patterns = [\n",
    "    [{\"POS\": \"ADV\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"PRON\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADV\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"VERB\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"ADJ\"}],\n",
    "    [{\"POS\": \"VERB\"}, {\"POS\": \"PART\"}, {\"POS\": \"ADP\"}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases:  [the Euclidean distance, two embedding vectors, Calculate the Euclidean, embedding vectors]\n",
      "POS Tags:  ['DET ADJ NOUN', 'NUM VERB NOUN', 'VERB DET ADJ', 'VERB NOUN']\n"
     ]
    }
   ],
   "source": [
    "import spacy, textacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "sample_text = 'Calculate the Euclidean distance between two embedding vectors.'\n",
    "sample_doc = nlp(sample_text)\n",
    "verb_phrases = textacy.extract.token_matches(sample_doc, vp_patterns)\n",
    "\n",
    "phrase_list = []\n",
    "tag_list = []\n",
    "\n",
    "# Extract Noun Phrases and corresponding pos tags\n",
    "for chunk in sample_doc.noun_chunks:\n",
    "    phrase_list.append(chunk)\n",
    "    tag_list.append(' '.join([t.pos_ for t in chunk]))\n",
    "# Print all Verb Phrase and corresponding pos tags\n",
    "for chunk in verb_phrases:\n",
    "    phrase_list.append(chunk)\n",
    "    tag_list.append(' '.join([t.pos_ for t in chunk]))\n",
    "\n",
    "print(\"Phrases: \", phrase_list)\n",
    "print(\"POS Tags: \", tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding vectors for pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'NNP': 0, 'DT': 1, 'VBG': 2, 'NN': 3, 'VB': 4, 'VBZ': 5}\n",
      "One-hot encoded vectors:\n",
      "[1. 0. 0. 1. 1. 0.]\n",
      "[0. 1. 1. 1. 0. 1.]\n",
      "[0. 0. 0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example corpus of sentences transformed into lists of POS tags\n",
    "corpus = [\n",
    "    ['VB', 'NN', 'NNP'],\n",
    "    ['DT', 'NN', 'VBZ', 'VBG'],\n",
    "    ['NN', 'NN', 'VB'],\n",
    "]\n",
    "\n",
    "# Step 1: Collect all unique POS tags to create a vocabulary\n",
    "all_pos_tags = set(tag for sentence in corpus for tag in sentence)\n",
    "\n",
    "# Step 2: Create a mapping from each POS tag to a unique index\n",
    "pos_to_index = {tag: idx for idx, tag in enumerate(all_pos_tags)}\n",
    "\n",
    "# Step 3: Generate one-hot encoded vectors\n",
    "def one_hot_encode(pos_list, pos_to_index):\n",
    "    vector = np.zeros(len(pos_to_index))\n",
    "    for pos in pos_list:\n",
    "        if pos in pos_to_index:\n",
    "            vector[pos_to_index[pos]] = 1\n",
    "    return vector\n",
    "\n",
    "# Example usage\n",
    "encoded_corpus = [one_hot_encode(sentence, pos_to_index) for sentence in corpus]\n",
    "\n",
    "print(\"Vocabulary:\", pos_to_index)\n",
    "print(\"One-hot encoded vectors:\")\n",
    "for vec in encoded_corpus:\n",
    "    print(vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy/Desktop/RAP/SHINE-EMNLP21/.shine/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "phrase_list = ['play an active role', 'participate actively', 'active lifestyle']\n",
    "\n",
    "model = SentenceTransformer('whaleloops/phrase-bert')\n",
    "phrase_embs = model.encode(phrase_list)\n",
    "[p1, p2, p3] = phrase_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Adjacency Matrix:\n",
      "[[0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_dependency_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    dependencies = [(token.text, token.head.text, token.dep_) for token in doc]\n",
    "    return dependencies, [token.text for token in doc]\n",
    "\n",
    "def create_adjacency_matrix(sentence):\n",
    "    dependencies, words = get_dependency_parse(sentence)\n",
    "    word_index = {word: i for i, word in enumerate(words)}\n",
    "    n = len(words)\n",
    "    \n",
    "    adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    for word, head, dep in dependencies:\n",
    "        if word != head:  # Skip self-loops\n",
    "            adjacency_matrix[word_index[head]][word_index[word]] = 1\n",
    "    \n",
    "    return adjacency_matrix, words\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "adj_matrix, words = create_adjacency_matrix(sentence)\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adj_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent shape (1, 100)\n",
      "entity_emb_cos 0.9999999999999999\n",
      "ent 1\n",
      "entities ['george washington']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "ent2id_new = json.load(open('./pretrained_emb/NELL_KG/ent2ids_refined', 'r'))        \n",
    "ent_mapping = {} \n",
    "entity_set = set()\n",
    "adj_ent_index = []\n",
    "\n",
    "def get_adj_ent_index(query, ent_mapping, ent2id_new):\n",
    "    # named entity recognition\n",
    "    np_list = []\n",
    "    ent_list = []\n",
    "    index = []\n",
    "    \n",
    "    # extract NP first\n",
    "    doc = nlp(query)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        np_list.append(chunk.text)\n",
    "    \n",
    "    # for every word in the NER dictionary\n",
    "    for key in ent2id_new.keys(): \n",
    "        # check if the word is in the text\n",
    "        if key in np_list: \n",
    "            # check if word is already in the mapping dict\n",
    "            if key not in ent_mapping: \n",
    "                # add word to ent_list\n",
    "                ent_list.append(key)\n",
    "                # add word to mapping dict as word:idx_in_ent_list\n",
    "                ent_mapping[key] = len(ent_mapping)\n",
    "                # update the set\n",
    "                entity_set.update(ent_list)\n",
    "            if ent_mapping[key] not in index: \n",
    "                index.append(ent_mapping[key])\n",
    "    # entity adjacency (index) matrix: list[list] of entities present in the sentences\n",
    "    adj_ent_index.append(index)\n",
    "\n",
    "\n",
    "sample_query = ['the quick brown fox jumps over the lazy dog', 'who is george washington']\n",
    "for sent in sample_query:\n",
    "    get_adj_ent_index(sent, ent_mapping, ent2id_new)\n",
    "# json.dump([adj_ent_index, ent_mapping],\n",
    "#           open('./{}_data/index_and_mapping.json'.format(dataset_name), 'w'), ensure_ascii=False)\n",
    "ent_emb = []\n",
    "TransE_emb_file = np.loadtxt('./pretrained_emb/NELL_KG/entity2vec.TransE')\n",
    "TransE_emb = []\n",
    "\n",
    "for i in range(len(TransE_emb_file)):\n",
    "    TransE_emb.append(list(TransE_emb_file[i, :]))\n",
    "\n",
    "rows = []\n",
    "data = []\n",
    "columns = []\n",
    "\n",
    "max_num = len(ent_mapping)\n",
    "# creating a coo format for matrix of adj_ent_index\n",
    "for sent_i, indices in enumerate(adj_ent_index):\n",
    "    for index in indices:\n",
    "        data.append(1)\n",
    "        rows.append(sent_i)\n",
    "        columns.append(index)\n",
    "\n",
    "# create a matrice of ones and zeros\n",
    "# ones correspond to (sentence_index, entity_index) i.e. which entities are present in the sentence\n",
    "adj_ent = coo_matrix((data, (rows, columns)), shape=(len(adj_ent_index), max_num))\n",
    "# for entity in entity mapping\n",
    "for key in ent_mapping.keys():\n",
    "    # add embedding to ent_emb\n",
    "    ent_emb.append(TransE_emb[ent2id_new[key]])\n",
    "\n",
    "ent_emb = np.array(ent_emb)\n",
    "print('ent shape', ent_emb.shape)\n",
    "ent_emb_normed = ent_emb / np.sqrt(np.square(ent_emb).sum(-1, keepdims=True))\n",
    "adj_emb = np.matmul(ent_emb_normed, ent_emb_normed.transpose())\n",
    "print('entity_emb_cos', np.mean(np.mean(adj_emb, -1)))\n",
    "# pkl.dump(np.array(ent_emb), open('./{}_data/entity_emb.pkl'.format(dataset_name), 'wb'))\n",
    "# pkl.dump(adj_ent, open('./{}_data/adj_query2entity.pkl'.format(dataset_name), 'wb'))\n",
    "\n",
    "entity_nodes = list(entity_set)\n",
    "\n",
    "print('ent', len(entity_nodes))\n",
    "print('entities', entity_nodes)\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent shape (6, 100)\n",
      "entity_emb_cos 0.18653110780888263\n",
      "ent 6\n",
      "entities ['fox', 'jump', 'row', 'quick', 'dog', 'brown']\n"
     ]
    }
   ],
   "source": [
    "sample_query = 'the quick brown fox jumps over the lazy dog'\n",
    "get_adj_ent_index(sample_query, ent_mapping, ent2id_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example corpus of sentences\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the dog and cat are friends\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "\n",
    "# 1. Collect all unique words in the corpus\n",
    "word_set = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "print(\"Word Set:\", word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a mapping from each word to a unique index\n",
    "word_to_index = {word: idx for idx, word in enumerate(word_set)}\n",
    "print(\"Word to Index Mapping:\", word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Compute co-occurrence matrix\n",
    "co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "window_size = 2\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(sentence), i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                co_occurrence_matrix[word][sentence[j]] += 1\n",
    "\n",
    "# Convert co-occurrence matrix to DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix).fillna(0)\n",
    "\n",
    "# Compute Positive Pointwise Mutual Information (PPMI) values\n",
    "total_sum = co_occurrence_df.values.sum()\n",
    "word_counts = co_occurrence_df.sum(axis=1)\n",
    "ppmi_matrix = np.maximum(\n",
    "    np.log((co_occurrence_df.values * total_sum) / (word_counts.values[:, None] * word_counts.values[None, :])),\n",
    "    0\n",
    ")\n",
    "np.fill_diagonal(ppmi_matrix, 0)\n",
    "print(\"PPMI Matrix:\\n\", ppmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate word embeddings using Word2Vec\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "word_vectors = np.array([model.wv[word] for word in word_set])\n",
    "print(\"Word Embeddings Shape:\", word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Example corpus of sentences\u001b[39;00m\n\u001b[1;32m     12\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe cat sat on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe dog chased the cat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe dog and cat are friends\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1131\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Example corpus of sentences\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the dog and cat are friends\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "\n",
    "# 1. Collect all unique words in the corpus\n",
    "word_set = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "\n",
    "# 2. Create a mapping from each word to a unique index\n",
    "word_to_index = {word: idx for idx, word in enumerate(word_set)}\n",
    "\n",
    "# 3. Compute the co-occurrence matrix\n",
    "co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "window_size = 2\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(sentence), i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                co_occurrence_matrix[word][sentence[j]] += 1\n",
    "\n",
    "# Convert co-occurrence matrix to DataFrame\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix).fillna(0)\n",
    "\n",
    "# Compute Positive Pointwise Mutual Information (PPMI) values\n",
    "total_sum = co_occurrence_df.values.sum()\n",
    "word_counts = co_occurrence_df.sum(axis=1)\n",
    "ppmi_matrix = np.maximum(\n",
    "    np.log((co_occurrence_df.values * total_sum) / (word_counts.values[:, None] * word_counts.values[None, :])),\n",
    "    0\n",
    ")\n",
    "np.fill_diagonal(ppmi_matrix, 0)\n",
    "\n",
    "# 4. Generate word embeddings using Word2Vec\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "word_vectors = np.array([model.wv[word] for word in word_set])\n",
    "\n",
    "# 5. Create a list of torch_geometric.data.Data graphs for each sentence in the corpus\n",
    "graphs = []\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    G = nx.Graph()\n",
    "    node_indices = [word_to_index[word] for word in sentence]\n",
    "    G.add_nodes_from(node_indices)\n",
    "    \n",
    "    edges = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for i, word1 in enumerate(sentence):\n",
    "        for j in range(i + 1, len(sentence)):\n",
    "            word2 = sentence[j]\n",
    "            if word1 != word2:\n",
    "                ppmi_value = ppmi_matrix[word_to_index[word1], word_to_index[word2]]\n",
    "                if ppmi_value > 0:\n",
    "                    edges.append((word_to_index[word1], word_to_index[word2]))\n",
    "                    edge_weights.append(ppmi_value)\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n",
    "    \n",
    "    x = torch.tensor([model.wv[word] for word in sentence], dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    graphs.append(data)\n",
    "\n",
    "# Create DataLoader for torch_geometric\n",
    "data_loader = DataLoader(graphs, batch_size=2, shuffle=True)\n",
    "\n",
    "# Example usage of DataLoader\n",
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    print(\"Batch x shape:\", batch.x.shape)\n",
    "    print(\"Batch edge_index shape:\", batch.edge_index.shape)\n",
    "    print(\"Batch edge_attr shape:\", batch.edge_attr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issue for data\n",
    "- structure of node embeddings  (num_nodes, emb_size)???\n",
    "    - PMI: for word nodes: use glove\n",
    "    - Entity: use TransE\n",
    "    - Dependency parsing??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data for demonstration\n",
    "def generate_random_graphs(num_nodes, num_node_features, num_classes, num_graphs):\n",
    "    x = torch.rand((num_nodes, num_node_features), dtype=torch.float)  # Node features\n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2), dtype=torch.long)  # Edges\n",
    "    y = torch.randint(0, num_classes, (num_nodes,), dtype=torch.long)  # Node labels\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create datasets for 5 separate graphs\n",
    "datasets = [generate_random_graphs(num_nodes=10, num_node_features=5, num_classes=3) for _ in range(100)]\n",
    "\n",
    "dataloaders = [DataLoader(dataset, batch_size=16, shuffle=True) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10]),\n",
       " Data(x=[10, 5], edge_index=[2, 20], y=[10])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "# Define a single GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "# Final model combining multiple GCNs and a classification layer\n",
    "class MultiGCNClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_gncs, num_classes):\n",
    "        super(MultiGCNClassifier, self).__init__()\n",
    "        self.gcns = torch.nn.ModuleList([GCN(in_channels, hidden_channels, out_channels) for _ in range(num_gncs)])\n",
    "        self.linear = torch.nn.Linear(out_channels * num_gncs, num_classes)\n",
    "\n",
    "    def forward(self, graphs):\n",
    "        embeddings = []\n",
    "        for i, graph in enumerate(graphs):\n",
    "            x, edge_index = graph.x, graph.edge_index\n",
    "            embeddings.append(self.gcns[i](x, edge_index))\n",
    "        \n",
    "        # Concatenate the embeddings from each GCN\n",
    "        concatenated = torch.cat(embeddings, dim=1)\n",
    "        out = self.linear(concatenated)\n",
    "        return F.log_softmax(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, optimizer, and loss function\n",
    "num_gncs = 5\n",
    "model = MultiGCNClassifier(in_channels=5, hidden_channels=16, out_channels=16, num_gncs=num_gncs, num_classes=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0893805027008057\n",
      "Epoch 2, Loss: 1.1288957595825195\n",
      "Epoch 3, Loss: 1.1174507141113281\n",
      "Epoch 4, Loss: 1.0951950550079346\n",
      "Epoch 5, Loss: 1.0923782587051392\n",
      "Epoch 6, Loss: 1.0871156454086304\n",
      "Epoch 7, Loss: 1.090480089187622\n",
      "Epoch 8, Loss: 1.11361563205719\n",
      "Epoch 9, Loss: 1.0942275524139404\n",
      "Epoch 10, Loss: 1.1013057231903076\n",
      "Epoch 11, Loss: 1.0750558376312256\n",
      "Epoch 12, Loss: 1.0790274143218994\n",
      "Epoch 13, Loss: 1.1071563959121704\n",
      "Epoch 14, Loss: 1.0836830139160156\n",
      "Epoch 15, Loss: 1.0988441705703735\n",
      "Epoch 16, Loss: 1.1076276302337646\n",
      "Epoch 17, Loss: 1.1086926460266113\n",
      "Epoch 18, Loss: 1.1190255880355835\n",
      "Epoch 19, Loss: 1.0933531522750854\n",
      "Epoch 20, Loss: 1.0927650928497314\n",
      "Accuracy: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for batches in zip(*dataloaders):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process each batch through the respective GCN\n",
    "        out = model(batches)\n",
    "        \n",
    "        # Use the labels for first graph of the batch for loss calculation\n",
    "        loss = F.nll_loss(out, batches[0].y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for batches in zip(*dataloaders):\n",
    "    out = model(batches)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct += (pred == batches[0].y).sum().item()\n",
    "    total += batches[0].num_nodes\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".shine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
