{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jy/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocess/trec_split.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_data = data['train']\n",
    "test_data = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'How did serfdom develop in and then leave Russia ?',\n",
       " 'coarse_label': 2,\n",
       " 'fine_label': 26}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to form any kinds of graph, you first need to go through the entire corpus and obtain {idx:node}. we first do this for word, then pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaning text\n",
    "import string\n",
    "def clean_str(sentence ,use=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    if not use: return sentence\n",
    "\n",
    "    sentence = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" \\'s\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" \\'ve\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" n\\'t\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" \\'re\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" \\'d\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" \\'ll\", sentence)\n",
    "    sentence = re.sub(r\",\", \" , \", sentence)\n",
    "    sentence = re.sub(r\"!\", \" ! \", sentence)\n",
    "    sentence = re.sub(r\"\\(\", \" \\( \", sentence)\n",
    "    sentence = re.sub(r\"\\)\", \" \\) \", sentence)\n",
    "    sentence = re.sub(r\"\\?\", \" \\? \", sentence)\n",
    "    sentence = re.sub(r\"\\s{2,}\", \" \", sentence)\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return sentence.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI Adjacency Matrix:\n",
      "[[0.         2.83321334 2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.83321334 0.         2.14006616 1.73460106 2.83321334 2.83321334]\n",
      " [2.14006616 2.14006616 0.         2.14006616 2.14006616 2.14006616]\n",
      " [1.73460106 1.73460106 2.14006616 0.         1.73460106 1.73460106]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 0.         2.83321334]\n",
      " [2.83321334 2.83321334 2.14006616 1.73460106 2.83321334 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    unique_words = set()\n",
    "    word_count = Counter()\n",
    "    pair_count = defaultdict(int)\n",
    "    total_words = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        line = clean_str(line)\n",
    "        words = line.split()\n",
    "        total_words += len(words)\n",
    "        word_count.update(words)\n",
    "        for i, word in enumerate(words):\n",
    "            unique_words.add(word)\n",
    "            for j in range(i + 1, len(words)):\n",
    "                pair = tuple(sorted([word, words[j]]))\n",
    "                pair_count[pair] += 1\n",
    "    \n",
    "    word_prob = {word: count / total_words for word, count in word_count.items()}\n",
    "    pair_prob = {pair: count / total_words for pair, count in pair_count.items()}\n",
    "    \n",
    "    return word_prob, pair_prob, unique_words\n",
    "\n",
    "def calculate_pmi(word_prob, pair_prob, word1, word2):\n",
    "    pair = tuple(sorted([word1, word2]))\n",
    "    if pair in pair_prob and word1 in word_prob and word2 in word_prob:\n",
    "        pmi = log(pair_prob[pair] / (word_prob[word1] * word_prob[word2]))\n",
    "        return pmi\n",
    "    return 0.0\n",
    "\n",
    "def create_pmi_matrix(sentence, word_prob, pair_prob, word_index):\n",
    "    words = clean_str(sentence).split()\n",
    "    n = len(words)\n",
    "    pmi_matrix = np.zeros((n, n))\n",
    "    node_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            node_list.append(word_index[word])\n",
    "        else:\n",
    "            node_list.append(-1)\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pmi = calculate_pmi(word_prob, pair_prob, words[i], words[j])\n",
    "            pmi_matrix[i, j] = pmi\n",
    "            pmi_matrix[j, i] = pmi  # PMI matrix is symmetric\n",
    "    \n",
    "    return pmi_matrix, node_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"Another line; with more: punctuation.\",\n",
    "    \"Is this working? Yes, it is!\"\n",
    "]\n",
    "\n",
    "word_prob, pair_prob, unique_words = process_corpus(corpus)\n",
    "word_index = {word: index for index, word in enumerate(sorted(unique_words))}\n",
    "\n",
    "sentence = \"Hello world, this is a test\"\n",
    "pmi_matrix, node_list = create_pmi_matrix(sentence, word_prob, pair_prob, word_index)\n",
    "print(\"PMI Adjacency Matrix:\")\n",
    "print(pmi_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'another': 1,\n",
       " 'hello': 2,\n",
       " 'is': 3,\n",
       " 'it': 4,\n",
       " 'line': 5,\n",
       " 'more': 6,\n",
       " 'punctuation': 7,\n",
       " 'test': 8,\n",
       " 'this': 9,\n",
       " 'with': 10,\n",
       " 'working': 11,\n",
       " 'world': 12,\n",
       " 'yes': 13}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same for pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How did serfdom develop in and then leave Russia ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    unique_tags = set()\n",
    "    tag_count = Counter()\n",
    "    tag_pair_count = defaultdict(int)\n",
    "    total_tags = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        line = clean_str(line)\n",
    "        # get pos tags for words in the query\n",
    "        tags = [one[1].lower() for one in nltk.pos_tag(nltk.word_tokenize(line))]\n",
    "        if '' in tags:\n",
    "            print(line)\n",
    "        tags = line.split()\n",
    "        total_tags += len(tags)\n",
    "        tag_count.update(tags)\n",
    "        for i, tag in enumerate(tags):\n",
    "            unique_tags.add(tag)\n",
    "            for j in range(i + 1, len(tags)):\n",
    "                pair = tuple(sorted([tag, tags[j]]))\n",
    "                tag_pair_count[pair] += 1\n",
    "    \n",
    "    tag_prob = {tag: count / total_tags for tag, count in tag_count.items()}\n",
    "    pair_prob = {pair: count / total_tags for pair, count in tag_pair_count.items()}\n",
    "    \n",
    "    return word_prob, pair_prob, unique_words\n",
    "\n",
    "def calculate_pmi(word_prob, pair_prob, word1, word2):\n",
    "    pair = tuple(sorted([word1, word2]))\n",
    "    if pair in pair_prob and word1 in word_prob and word2 in word_prob:\n",
    "        pmi = log(pair_prob[pair] / (word_prob[word1] * word_prob[word2]))\n",
    "        return pmi\n",
    "    return 0.0\n",
    "\n",
    "def create_pmi_matrix(sentence, word_prob, pair_prob, word_index):\n",
    "    words = clean_str(sentence).split()\n",
    "    n = len(words)\n",
    "    pmi_matrix = np.zeros((n, n))\n",
    "    node_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            node_list.append(word_index[word])\n",
    "        else:\n",
    "            node_list.append(-1)\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            pmi = calculate_pmi(word_prob, pair_prob, words[i], words[j])\n",
    "            pmi_matrix[i, j] = pmi\n",
    "            pmi_matrix[j, i] = pmi  # PMI matrix is symmetric\n",
    "    \n",
    "    return pmi_matrix, node_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"Another line; with more: punctuation.\",\n",
    "    \"Is this working? Yes, it is!\"\n",
    "]\n",
    "\n",
    "word_prob, pair_prob, unique_words = process_corpus(corpus)\n",
    "word_index = {word: index for index, word in enumerate(sorted(unique_words))}\n",
    "\n",
    "sentence = \"Hello world, this is a test\"\n",
    "pmi_matrix, node_list = create_pmi_matrix(sentence, word_prob, pair_prob, word_index)\n",
    "print(\"PMI Adjacency Matrix:\")\n",
    "print(pmi_matrix)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".shine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
